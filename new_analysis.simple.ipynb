{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating Exoplanet Occurrence Rates: Survey Sensitivity, Detection Efficiency, and Statistical Modeling\n",
    "\n",
    "---\n",
    "\n",
    "### Purpose and Content\n",
    "This notebook is focused on the statistical analysis of exoplanet detection rates, specifically for a simulated or real survey. The analysis involves:\n",
    "* **Reading and processing a dataset** of detected planetary events, including their properties such as mass, semi-major axis, and detection weights.\n",
    "* **Applying selection cuts** to the data to filter for events that meet certain criteria (e.g., signal-to-noise, in-season detection).\n",
    "* **Correcting for survey geometry and false positive rates** using external data (e.g., covering factors).\n",
    "* **Calculating and visualizing detection efficiencies** as a function of planet mass and semi-major axis.\n",
    "* **Building and plotting histograms** of key parameters (e.g., semi-major axis, event times, impact parameters).\n",
    "* **Computing the survey sensitivity** and the expected number of detections for different planet populations.\n",
    "* **Fitting occurrence rate models** (e.g., power laws in mass and semi-major axis) to the observed planet sample using likelihood and MCMC methods.\n",
    "* **Visualizing the results** with various plots, including detection efficiency maps and parameter distributions.\n",
    "\n",
    "### Key Methods and Libraries\n",
    "* **Python scientific stack:** pandas, numpy, matplotlib, scipy, lmfit, emcee, corner.\n",
    "* **Statistical modeling:** Maximum likelihood and MCMC for fitting planet occurrence rates.\n",
    "* **Survey simulation:** Functions to draw samples, apply weights, and correct for observational biases.\n",
    "\n",
    "### Scientific Context\n",
    "\n",
    "This notebook implements a standard pipeline for calculating exoplanet occurrence rates from a microlensing survey, heavily influenced by methods from [Suzuki et al. (2016)](https://ui.adsabs.harvard.edu/abs/2016ApJ...833..145S/abstract) and [Penny et al. (2019)](https://ui.adsabs.harvard.edu/abs/2019ApJS..241....3P/abstract).\n",
    "\n",
    "The core idea is to take a simulated dataset of planetary events, which have been corrected for observational biases like seasonal gaps, and determine the underlying true distribution of planets. We do this by:\n",
    "1.  **Filtering** the raw simulation output to create clean samples of all possible events (`dat`) and only those actually detected (`det`).\n",
    "2.  **Calculating the survey's detection efficiency**, which creates a map of how sensitive the survey is to planets of different masses (`m_p`) and semi-major axes (`a`).\n",
    "3.  **Defining an occurrence rate model** (e.g., a power law) that describes the planet distribution.\n",
    "4.  **Fitting this model** to the detected planets using a Maximum Likelihood Estimate (MLE) and exploring the parameter space with a Markov Chain Monte Carlo (MCMC) to find the best-fit parameters and their uncertainties.\n",
    "\n",
    "The final goal is to produce a measurement like `\u03b7_Earth` (eta-Earth), the fraction of stars that host an Earth-like planet.\n",
    "\n",
    "### Outputs\n",
    "\n",
    "* Plots of the spatial and parameter distributions of detected events.\n",
    "* Detection efficiency maps in planet mass and semi-major axis space.\n",
    "* Calculation of the total number of expected events after corrections.\n",
    "* Preparation for or execution of statistical fits to infer underlying planet population parameters.\n",
    "\n",
    "### Overview\n",
    "\n",
    "This notebook is a comprehensive analysis pipeline for estimating exoplanet occurrence rates from a survey,  including data processing, bias correction, statistical modeling, and visualization. It is for used in the context of a microlensing survey simulation or data analysis, with a focus on understanding the true distribution of exoplanets after accounting for observational biases.\n",
    "\n",
    "### Known Issues\n",
    "\n",
    "There is an unverified `scalefac ` in section [\"Applying the Survey Covering Factor and Final Weight Corrections\"](#applying-the-survey-covering-factor-and-final-weight-corrections). \n",
    "Without this variable to provide meaningful normalization, this notebook is only useful for pipeline validation.\n",
    "`scalefac` converts \"weight per simulated star\" to \"planets per real star\"\n",
    "\n",
    "It requires knowing:\n",
    "\n",
    "* Simulated stellar density (stars/sq.deg in simulation)\n",
    "* Actual Galactic stellar density (stars/sq.deg in reality)\n",
    "\n",
    "Without this, occurrence rates remain relative but not absolute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Use This Notebook (The General Case)\n",
    "\n",
    "---\n",
    "\n",
    "This notebook is designed to guide you through the process of analyzing microlensing exoplanet survey data, estimating occurrence rates, and understanding your survey\u2019s sensitivity and results.  \n",
    "It is structured to be beginner-friendly, with clear explanations and example outputs at each step.\n",
    "\n",
    "### Step-by-Step Instructions\n",
    "\n",
    "1. **Set Up Your Environment**\n",
    "   - Make sure you have all the required Python packages installed (see the `environment.yml` or `requirements.txt`).\n",
    "   - Place your data files (e.g., HDF5 event files, covering factor files) in the correct locations as specified in the code cells.\n",
    "\n",
    "2. **Load and Prepare Your Data**\n",
    "   - Run the cells that load your planet event data and apply any necessary preprocessing.\n",
    "   - The notebook will add extra columns (like insolation and season flags) to your data for later analysis.\n",
    "\n",
    "3. **Apply Survey Corrections**\n",
    "   - The notebook will automatically correct for survey coverage, false positives, and other effects to ensure your results are fair and accurate.\n",
    "\n",
    "4. **Filter and Visualize Your Data**\n",
    "   - Use the provided cells to filter your data based on detection criteria.\n",
    "   - Visualize the distribution of your events in various ways (sky position, distance, time, impact parameter, etc.).\n",
    "   - Example plots are provided so you can check if your outputs look reasonable.\n",
    "\n",
    "5. **Calculate Survey Sensitivity**\n",
    "   - The notebook computes how sensitive your survey is to different types of planets (by mass, distance, and insolation).\n",
    "   - Visualize these sensitivities with heatmaps and summary plots.\n",
    "\n",
    "6. **Fit Occurrence Rate Models**\n",
    "   - Use maximum likelihood estimation (MLE) to fit your occurrence rate model to the data.\n",
    "   - Check the fit by comparing model predictions to observed distributions.\n",
    "\n",
    "7. **Run MCMC for Uncertainties**\n",
    "   - Set up and run Markov Chain Monte Carlo (MCMC) to explore the full range of model parameters that fit your data.\n",
    "   - Use the corner plots and model curve plots to check for convergence and parameter correlations.\n",
    "\n",
    "8. **Interpret Your Results**\n",
    "   - Use the best-fit parameters and their uncertainties to draw scientific conclusions about planet populations in your survey.\n",
    "   - Compare your results to example outputs and literature values as a sanity check.\n",
    "\n",
    "### Tips for Success\n",
    "\n",
    "- **Read the markdown explanations** in each section\u2014they\u2019re written to be insultingly accessible even if you\u2019re new to this kind of analysis.\n",
    "- **Check the example outputs** to make sure your results look reasonable at each step.\n",
    "- **Use the function links** (e.g., [`ssn`](#what-does-ssn-do)) to jump to detailed explanations of what each function does.\n",
    "- **If you get stuck or see unexpected results**, review the diagnostic plots and pro tips, or consult the documentation for the relevant packages (like [`emcee`](https://emcee.readthedocs.io/en/stable/)).\n",
    "\n",
    "### Customization\n",
    "\n",
    "- You can change the data file paths, parameter ranges, and model forms to suit your own survey or science goals.\n",
    "- If you want to analyze a different dataset, just update the relevant file paths and rerun the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Use this Notebook to Check Your Simulation\n",
    "\n",
    "---\n",
    "\n",
    "You made the simulation, so you know the \"true\" occurrence rate parameters that you programmed in. The entire point of running this notebook is to see if the analysis can correctly figure out those parameters from the data you generated.\n",
    "\n",
    "Follow these steps:\n",
    "\n",
    "1.  **Set Your Input File:** In the [**\"Loading Your Planet Data\"** section](#loading-your-planet-data), change the `filename` variable to point to your Gulls simulation output file.\n",
    "    ```python\n",
    "    # Change this line to your file:\n",
    "    filename = '/path/to/your/gulls_simulation_output.hdf5'\n",
    "    ```\n",
    "\n",
    "2.  **Enter YOUR \"True\" Parameters:** This is the most important step. In the section [**\"Setting Up True Model Parameters for Testing,\"**](#setting-up-true-model-parameters-for-testing) find the `true_params` dictionary. You **must** update the values for `logC_true`, `n_true`, and `m_true` to match the parameters you used to create your simulation data.\n",
    "\n",
    "    ```python\n",
    "    # UPDATE THESE VALUES:\n",
    "    n_true = -0.9      # Whatever your true mass slope was\n",
    "    m_true = 0.5       # Whatever your true SMA slope was\n",
    "    logC_true = np.log10(1.5)  # Whatever your true normalization was\n",
    "    \n",
    "    true_params = {'logC':logC_true,\n",
    "                   'n':n_true,\n",
    "                   'm':m_true}\n",
    "    ```\n",
    "\n",
    "3.  **Run the Notebook:** Run all the cells from top to bottom.\n",
    "\n",
    "4.  **Check the Results:** Go to the [final plots](#visualizing-mcmc-results-with-a-corner-plot):\n",
    "    * **In the distribution plots:** The \"Fit Rate\" line should land right on top of the \"True Rate\" line. If it doesn't, something is wrong.\n",
    "    * **In the corner plot:** The blue lines showing the \"truths\" should go right through the middle of your data blobs (the posteriors). This proves the MCMC found the right answer.\n",
    "\n",
    "This turns the notebook from a simple analysis tool into a validation tool. It's no longer just giving you an answer; it's showing you if the answer is *correct*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How this Notebook Relates to the Roman Science Requrements and What Needs Doing.\n",
    "\n",
    "---\n",
    "\n",
    "> **RST Level-1 Science Requirements** - Those highlighted are not verified at all. Those verified do so at \\~20% margin with old slew/settle times and assuming 6 x 72 day seasons.\n",
    "> * **EML 2.0.1:** RST shall be capable of measuring the mass function of exoplanets with masses in the range $1M_{\\oplus} < m < 30M_{\\rm{Jupiter}}$ and orbital semi-major axes $\\ge 1 \\rm{au}$ to better than 15% per decade in mass.\n",
    "> * **EML 2.0.2:** RST shall be capable of measuring the frequency of bound exoplanets with masses in the range $0.1M_{\\oplus} < m < 0.3M_{\\oplus}$ to better than 25%.\n",
    "> * [**EML 2.0.3:** RST shall be capable of determining the masses of, and distances to, host stars of 40% of the detected planets with a precision of 20% or better.]()\n",
    "> * **EML 2.0.4:** RST shall be capable of measuring the frequency of free floating planetary- mass objects in the Galaxy from Mars to 10 Jupiter masses. If there is $1M_{\\oplus}$ free-floating planet per star, measure this frequency to better than 25%.\n",
    "> * [**EML 2.0.5:** RST shall be capable of estimating $\\eta_{\\oplus}$ (the frequency of planets orbiting FGK stars with mass ratio and estimated projected semimajor axis within 20% of the Earth-Sun system) to a precision of 0.2 dex via extrapolation from larger and longer-period planets.]()\n",
    "\n",
    "The whole point of this exercise is to check off those science requirements for the Roman telescope. The notebook is just the tool. You need to connect the dots.\n",
    "\n",
    "Here's the breakdown on how this notebook helps you tackle that list.\n",
    "\n",
    "### How This Notebook Answers Your Science Questions:\n",
    "\n",
    "#### 1\\. For Measuring the Planet Mass Function (Requirement EML 2.0.1)\n",
    "\n",
    "  * **What it is:** This is the main job of the notebook. The goal is to figure out how common planets of different masses are.\n",
    "  * **What you needs to do:**\n",
    "    1.  Run the entire notebook to get a sample of *detected* planets.\n",
    "    2.  Pay close attention to the **\"Modelling the Occurance Rates\"** and **\"Re-fitting with MCMC\"** sections. The MCMC fit gives the best-fit parameters for the planet population, specifically the power-law slope for mass (the parameter `n`).\n",
    "    3.  The final corner plot shows the measurement and its uncertainty. This directly addresses the \"better than 15% per decade in mass\" part. The plots in **\"Occurance Rate Fitting Results\"** are her proof that the model fits.\n",
    "\n",
    "#### 2\\. For Finding the Frequency of Low-Mass Planets (Requirement EML 2.0.2)\n",
    "\n",
    "  * **What it is:** This is just a specific slice of the big mass function from the first requirement.\n",
    "  * **What you needs to do:**\n",
    "    1.  Once you have the final, best-fit model from the MCMC run, you needs to integrate that function over the specified mass range ($0.1 M\\_{\\\\oplus}$ to $0.3 M\\_{\\\\oplus}$).\n",
    "    2.  This isn't a pre-built cell, so you'll have to add a little bit of code at the end to do this calculation using the MCMC results. It's a direct outcome of the main analysis.\n",
    "\n",
    "#### 3\\. For Estimating \u03b7\u2295 (eta-Earth) (Requirement EML 2.0.5)\n",
    "\n",
    "  * **What it is:** This is the big one, the reason for the whole project. It's about finding Earth-like planets.\n",
    "  * **What you needs to do:**\n",
    "    1.  Just like with the low-mass planets, this requires taking the final fitted model from the MCMC.\n",
    "    2.  You'll need to integrate that model over the specific \"Earth-like\" box\u2014that means filtering by planet mass *and* by **insolation** (how much light the planet gets from its star).\n",
    "    3.  The notebook already calculates insolation for you in the [**\"Adding Extra Information to Your Data\"** cell](#adding-extra-information-to-your-data-insolation) and even makes a `mass-insolation` detection map. You have all the pieces; you just needs to put them together in a final calculation. The [**\"How to Interpret the Results\"** section](#how-to-interpret-the-results) points this out.\n",
    "\n",
    "#### 4\\. What This Notebook *Doesn't* Do (But Is Still Needed For)\n",
    "\n",
    "This part is crucial, so you doesn't waste your time looking for things that aren't there.\n",
    "\n",
    "  * **Host Star Masses (EML 2.0.3):** This notebook simulates the *detection* of planets, not the detailed follow-up measurements needed to determine the mass of the host stars. As the [Penny (2019)](https://ui.adsabs.harvard.edu/abs/2019ApJS..241....3P/abstract) paper explains, that requires measuring things like the lens flux or parallax over the course of the mission. This notebook's output\u2014the list of detected planets\u2014is the *input* for that separate analysis.\n",
    "  * **Free-Floating Planets (EML 2.0.4):** This analysis is explicitly for *bound* planets. [Penny (2019)](https://ui.adsabs.harvard.edu/abs/2019ApJS..241....3P/abstract) even says that free-floating planets are a different challenge that requires a separate paper. The method is similar, but the models and event selection would be different. You can't use this notebook for that requirement without significant changes and completely different simulation data.\n",
    "\n",
    "### The Key: Comparing Your Uncertainties to the Requirements\n",
    "\n",
    "The most important result from this notebook is not the best-fit value for a parameter\u2014it's the **uncertainty** on that value. The whole point of this exercise is to see if the final error bars from your MCMC fit are small enough to satisfy the requirements.\n",
    "\n",
    "Think of it like this:\n",
    "\n",
    "* The requirement says: \"measure the mass function... to **better than 15%**\" (EML 2.0.1).\n",
    "* Your MCMC will give you a result for the mass slope, something like: `n = -0.9 \u00b1 0.1`.\n",
    "* That `\u00b1 0.1` is your uncertainty. In this case, it's about an 11% measurement ($0.1 / 0.9 \\approx 11\\%$).\n",
    "* Since 11% is better than 15%, you've met the requirement. If your uncertainty was `\u00b1 0.2`, you would have failed.\n",
    "\n",
    "**Every requirement is a test of the *precision* of your final MCMC results.** Your job is to prove that this pipeline can produce measurements that are not just *right*, but *certain enough*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "---\n",
    "\n",
    " This notebook requires several Python packages that can be installed using conda or pip.\n",
    " \n",
    " Using conda (recommended):\n",
    " ```bash\n",
    " conda env create -f env.yml\n",
    " conda activate eta-earth-analysis\n",
    " ```\n",
    " \n",
    " Using pip:\n",
    " ```bash\n",
    " pip install -r requiremenets.txt\n",
    " ```\n",
    " \n",
    " The required packages are:\n",
    " - `pandas`: For data manipulation and analysis\n",
    " - `numpy`: For numerical computing\n",
    " - `matplotlib`: For plotting\n",
    " - `scipy`: For scientific computing\n",
    " - `lmfit`: For curve fitting and parameter estimation\n",
    " - `emcee`: For MCMC sampling\n",
    " - `corner`: For visualization of MCMC results\n",
    " - `jupyterlab`: For running this notebook\n",
    " - `ipykernel`: For Jupyter notebook support\n",
    " - `h5py`: For HDF5 file support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import scipy.integrate\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import lmfit\n",
    "import emcee\n",
    "import corner\n",
    "import timeit\n",
    "import os\n",
    "matplotlib.rcParams['mathtext.fontset'] = 'stix'\n",
    "matplotlib.rcParams['font.family'] = 'STIXGeneral'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Functions\n",
    "\n",
    "---\n",
    "\n",
    "### What does `draw_sample` do?\n",
    "\n",
    "Imagine you have a big list of events (like a list of planets, or lottery tickets). Each event has a \u201cweight\u201d\u2014think of this as how likely or important that event is. Some events are more likely to be picked than others.\n",
    "This function helps you make a new list by randomly picking events from your big list. But it doesn\u2019t pick them all with the same chance:\n",
    "\n",
    "* Events with a bigger weight are more likely to be picked.\n",
    "* Events with a smaller weight are less likely to be picked.\n",
    "\n",
    "#### Here\u2019s how it works, step by step:\n",
    "\n",
    "1. **Add up all the weights.** This gives you a sense of how many events you \u201cexpect\u201d to pick.\n",
    "2. **Decide how many to pick.** The function uses a bit of randomness (like rolling dice) to decide the actual number, based on the total weight.\n",
    "3. **Pick events, one by one.** Each time, it\u2019s more likely to pick an event with a bigger weight.\n",
    "4. **Make a new list.** The result is a new list of events, picked in a way that respects their weights.\n",
    "\n",
    "> **Why do this?**\n",
    ">\n",
    "> This is useful when you want to simulate what you might see in a real experiment, where some things are more likely to happen than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to draw a sample of events \n",
    "def draw_sample(df,weight_col='fwc'):\n",
    "    \"\"\"\n",
    "    Draw a Poisson-resampled subset of events using per-event weights.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing event information.\n",
    "    weight_col : str, optional\n",
    "        Name of the column in ``df`` containing the weights. These\n",
    "        weights are normalized and used as the selection probability.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Random sample of events drawn with replacement according to the\n",
    "        provided weights.\n",
    "    \"\"\"\n",
    "    # assumes weights are correct\n",
    "    # norm is the sum of weights (i.e., expected number of detections)\n",
    "    norm = np.sum(df[weight_col])\n",
    "    # probabilities for individual events are equal to normalized weights\n",
    "    probs = df[weight_col] / norm\n",
    "    # the number of events drawn is a poisson draw of the expected number of events\n",
    "    ndraw = np.random.poisson(lam=norm)\n",
    "    # draw a random subset of events to be in the sample. Should replace be true?\n",
    "    sample_idx = np.random.choice(df.shape[0],ndraw,replace=True,p=probs)\n",
    "    # generate a data frame of the sample from the original data frame                                                                                                                                                                                                \n",
    "    sample = df.iloc[sample_idx]\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does `ssn` do?\n",
    "\n",
    "Imagine you have a list of events, and each event happened at a certain time (let\u2019s call it `x`). You want to know if that time falls inside a \u201cseason\u201d (a special time period), or outside of it.\n",
    "\n",
    "This function takes a time (or a list of times) and tells you:\n",
    "\n",
    "* **If the time is inside a season:** It gives you a positive number (which season it is).\n",
    "* **If the time is outside a season:** It gives you a negative number (which season it\u2019s after).\n",
    "\n",
    "#### How does it work?\n",
    "\n",
    "* It checks the value of x and compares it to a bunch of cut-off numbers.\n",
    "* Depending on where x falls, it returns a number that tells you if it\u2019s in a season (positive) or not (negative), and which one.\n",
    "\n",
    "> **Why do this?**\n",
    ">\n",
    "> This is useful if you only care about events that happen during certain times (seasons), or if you want to filter out events that happen outside those times.\n",
    "\n",
    "> **In short:**\n",
    ">\n",
    "> You give it a time, it tells you if that time is in a special \u201cseason\u201d or not, and which one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssn(x):\n",
    "    \"\"\"\n",
    "    Determine whether a given time falls within an observing season.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array_like or float\n",
    "        Time(s) in days to evaluate.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    int or ndarray\n",
    "        Positive values denote the season number if ``x`` is in season.\n",
    "        Negative values indicate the preceding season when out of season.\n",
    "    \"\"\"\n",
    "        if np.isscalar(x):\n",
    "                if x<112.5:\n",
    "                        return 0\n",
    "                if x<=184.5:\n",
    "                        return 1\n",
    "                if x<292.25:\n",
    "                        return -1\n",
    "                if x<=364.25:\n",
    "                        return 2\n",
    "                if x<477.75:\n",
    "                        return -2\n",
    "                if x<=549.75:\n",
    "                        return 3\n",
    "                if x<1388.0:\n",
    "                        return -3\n",
    "                if x<=1460.0:\n",
    "                        return 4\n",
    "                if x<1573.5:\n",
    "                        return -4\n",
    "                if x<=1645.5:\n",
    "                        return 5\n",
    "                if x<1753.25:\n",
    "                        return -5\n",
    "                if x<=1825.25:\n",
    "                        return 6\n",
    "                return -6\n",
    "        else:\n",
    "                ret = x*0+99\n",
    "                ret[(ret==99) * (x<112.5)] = 0\n",
    "                ret[(ret==99) * (x<=184.5)] = 1\n",
    "                ret[(ret==99) * (x<292.25)] = -1\n",
    "                ret[(ret==99) * (x<=364.25)] = 2\n",
    "                ret[(ret==99) * (x<477.75)] = -2\n",
    "                ret[(ret==99) * (x<=549.75)] = 3\n",
    "                ret[(ret==99) * (x<1388.0)] = -3\n",
    "                ret[(ret==99) * (x<=1460.0)] = 4\n",
    "                ret[(ret==99) * (x<1573.5)] = -4\n",
    "                ret[(ret==99) * (x<=1645.5)] = 5\n",
    "                ret[(ret==99) * (x<1753.25)] = -5\n",
    "                ret[(ret==99) * (x<=1825.25)] = 6\n",
    "                ret[ret==99] = -6\n",
    "\n",
    "                return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does `FPF` do?\n",
    "\n",
    "Sometimes, when you find something (like a planet), it might actually be a \u201cfalse positive\u201d\u2014meaning it looks real, but it\u2019s not. This function helps you figure out how likely it is that a planet you found is actually a mistake (a false positive).\n",
    "\n",
    "#### How does it work?\n",
    "\n",
    "* It looks at the mass of the planet.\n",
    "* It uses a special formula (from a scientific paper) to calculate the chance that the planet is a false positive.\n",
    "* The answer is a number between 0 and 1. A bigger number means more likely to be a false positive; a smaller number means less likely.\n",
    "\n",
    "> **Why do this?**\n",
    ">\n",
    "> This helps you correct your results, so you don\u2019t count things that are probably not real planets.\n",
    "\n",
    "> **In short:**\n",
    ">\n",
    "> You give it a planet\u2019s mass, it tells you how likely it is that planet is actually just a mistake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FPF(data,mearth,mass_col=42):\n",
    "    \"\"\"\n",
    "    Apply the false positive fraction correction from Penny (2019).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pandas.DataFrame\n",
    "        Planet catalog containing a ``Planet_mass`` column.\n",
    "    mearth : float\n",
    "        Mass of Earth in Solar masses.\n",
    "    mass_col : int, optional\n",
    "        Unused placeholder kept for legacy reasons.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ndarray\n",
    "        False positive fraction for each planet.\n",
    "    \"\"\"\n",
    "    alpha = 1.25\n",
    "    beta = 2.7\n",
    "    return (1+10**(alpha*np.log10(data['Planet_mass']/mearth)+beta))**-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does `mass_func_arr` do?\n",
    "\n",
    "Suppose you want to know how common different kinds of planets are\u2014like, are big planets more common than small ones? Are planets far from their star more common than close ones? This function helps you figure that out.\n",
    "\n",
    "#### How does it work?\n",
    "\n",
    "* You give it a list of planets, and some numbers (called \u201cparameters\u201d) that describe how you think planet types are spread out.\n",
    "* For each planet, it looks at its mass and how far it is from its star.\n",
    "* It uses a math formula (called a \u201cpower law\u201d) with your parameters to calculate a number for each planet. This number tells you how likely that kind of planet is, according to your model.\n",
    "  $$ \\frac{d^2 N}{d\\log m_p  d\\log a} = C \\cdot \\left(\\frac{m_p}{M_\\oplus}\\right)^n \\cdot \\left(\\frac{a}{\\text{au}}\\right)^m $$\n",
    "\n",
    "> **Why do this?**\n",
    ">\n",
    "> This helps you build a picture of what kinds of planets are most common, based on their mass and distance from their star.\n",
    "\n",
    "> **In short:**\n",
    ">\n",
    "> You give it a bunch of planets and some model settings, and it tells you how common each planet type is, using a math formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mass_func_arr(pars,planet_sample,mearth=3e-6):\n",
    "    \"\"\"\n",
    "    Evaluate a power-law occurrence model for a set of planets.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pars : dict or sequence\n",
    "        Model parameters ``{'logC', 'n', 'm'}`` or ``[logC, n, m]``.\n",
    "    planet_sample : pandas.DataFrame or array_like\n",
    "        Planet properties or an array of log masses.\n",
    "    mearth : float, optional\n",
    "        Mass of Earth in Solar masses.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ndarray\n",
    "        Occurrence rate evaluated at each planet.\n",
    "    \"\"\"\n",
    "    # compute the mass function values for a set of pars given a dataframe of events\n",
    "    # this version is for a saturated power law in mass, single power law in SMA\n",
    "    #logC, logMbreak, n, m = theta \n",
    "    try:\n",
    "        M_planet = planet_sample['Planet_mass']/mearth\n",
    "        sma = planet_sample['Planet_semimajoraxis']\n",
    "    # this is to catch when an array of values (instead of data frame) is an argument, could be smarter\n",
    "    except:\n",
    "        M_planet = 10**planet_sample\n",
    "        sma = np.ones(len(M_planet))\n",
    "    \n",
    "    # set up return array\n",
    "    ret = np.ones(len(sma))\n",
    "    \n",
    "    \n",
    "    # unpack the pars into a dict if in list form\n",
    "    if type(pars) is list or type(pars) is np.ndarray:\n",
    "        #starting_mean = np.array([logC1_start,logMbreak_start,logC2_start,logMS2_start,n2_start,m_start])                                                                                                             \n",
    "        theta = {'logC':pars[0],\n",
    "                 'n':pars[1],\n",
    "                 'm':pars[2]}\n",
    "    # else pars is already a dictionary\n",
    "    else:\n",
    "        theta=pars\n",
    "\n",
    "    # compute rates\n",
    "    ret = 10**theta['logC'] * (M_planet)**theta['n'] * sma**theta['m']\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does `occ_arr` do?\n",
    "Let\u2019s say you want to make a map that shows how common different types of planets are, depending on their mass (how heavy they are) and their distance from their star. This function helps you fill in that map.\n",
    "\n",
    "#### How does it work?\n",
    "\n",
    "* You give it a grid of possible planet masses and distances (think of a big table with all the combinations).\n",
    "* You also give it some numbers (parameters) that describe your guess for how planet types are spread out.\n",
    "* For every spot in the grid (every combination of mass and distance), it uses a math formula to figure out how common a planet would be there, according to your model.\n",
    "\n",
    "> **Why do this?**\n",
    ">\n",
    "> This lets you see, all at once, which kinds of planets your model says should be common or rare, across a whole range of masses and distances.\n",
    "\n",
    "> **In short:**\n",
    "> \n",
    "> You give it a grid of planet types and some model settings, and it fills in the grid with numbers that say how common each type should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def occ_arr(pars,m,a,mearth=3e-6):\n",
    "    \"\"\"\n",
    "    Compute the occurrence rate on a mass--semi-major axis grid.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pars : dict or sequence\n",
    "        Model parameters as in :func:`mass_func_arr`.\n",
    "    m : ndarray\n",
    "        log10 planet masses.\n",
    "    a : ndarray\n",
    "        log10 semi-major axes.\n",
    "    mearth : float, optional\n",
    "        Mass of Earth in Solar masses.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ndarray\n",
    "        2D array of occurrence rates with shape ``(len(a), len(m))``.\n",
    "    \"\"\"\n",
    "    #logC, logMbreak, n, m = theta\n",
    "    # delog the grid point values for mass func calcs\n",
    "    M_planet = 10**m\n",
    "    sma = 10**a\n",
    "    # just making some arrays of ones\n",
    "    m_vals = (M_planet/M_planet).copy() \n",
    "    s_vals = (sma/sma).copy()\n",
    "    \n",
    "    # unpack the pars into a dict if in list form\n",
    "    if type(pars) is list or type(pars) is np.ndarray:\n",
    "        theta = {'logC':pars[0],\n",
    "                 'n':pars[1],\n",
    "                 'm':pars[2]}                                                                                                                                                                                         \n",
    "    else:\n",
    "        theta=pars\n",
    "\n",
    "    m_vals = 10**theta['logC'] * (M_planet)**theta['n']\n",
    "    s_vals = sma**theta['m']\n",
    "    return m_vals.reshape(1,-1)*s_vals.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCMC Functions\n",
    "\n",
    "---\n",
    "\n",
    "### What does `N_exp_real` do?\n",
    "\n",
    "Imagine you want to know: **\u201cIf my model is right, how many planets should I expect to actually find in my survey?\u201d**\n",
    "\n",
    "This function helps you figure that out.\n",
    "\n",
    "#### How does it work?\n",
    "\n",
    "It takes your model (the \u201ctheta\u201d numbers) and a map of how good your survey is at finding planets (the \u201csurvey sensitivity\u201d). It makes a big grid of all possible planet types (different masses and distances). For each spot in the grid, it figures out how common that planet should be (using your model) and how likely your survey is to find it. It adds up all those numbers, across the whole grid, to get the total number of planets you\u2019d expect to detect.\n",
    "\n",
    "> **Why do this?**\n",
    ">\n",
    "> This tells you, for your model and your survey, how many planets you should see\u2014so you can compare with what you actually found.\n",
    "\n",
    "> **In short:** \n",
    ">\n",
    "> You give it your model and your survey\u2019s abilities, and it tells you how many planets you should expect to find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def N_exp_real(theta,planet_sample,survey_sensitivity):\n",
    "    \"\"\"\n",
    "    Integrate the survey sensitivity to get the expected number of detections.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : dict or sequence\n",
    "        Occurrence rate parameters.\n",
    "    planet_sample : pandas.DataFrame\n",
    "        Sample of detected planets.\n",
    "    survey_sensitivity : ndarray\n",
    "        2D sensitivity grid.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Expected number of detected planets.\n",
    "    \"\"\"\n",
    "    # integrate the survey sensitivity to get the expected number of detections\n",
    "    # these should be arguments, but left in for now\n",
    "    a_low,a_high,m_low,m_high = 0.01,10.,0.01,10.\n",
    "    nbins = survey_sensitivity.shape[0]+1\n",
    "    # generate grid points to integrate over\n",
    "    m_bins = np.linspace(np.log10(m_low),np.log10(m_high),nbins,endpoint=True)\n",
    "    a_bins = np.linspace(np.log10(a_low),np.log10(a_high),nbins,endpoint=True)\n",
    "    m_bin_centers = (m_bins[1:]+m_bins[:-1])/2\n",
    "    a_bin_centers = (a_bins[1:]+a_bins[:-1])/2\n",
    "\n",
    "    # make a 2d array of the expected number of detections given the occurrence rate parameters being proposed (theta)\n",
    "    zz = occ_arr(theta,a_bin_centers.reshape(-1,1),m_bin_centers.reshape(1,-1))\n",
    "    # integrate over the occurrence rate grid\n",
    "    # the next line *SHOULD* work, it should just be a 2d integral. I haven't gotten a chance to retest it\n",
    "    #int1 = scipy.integrate.simps([scipy.integrate.simps(zz_x,a_bin_centers) for zz_x in zz*survey_sensitivity],m_bin_centers)\n",
    "    # split integral into two parts to clarify/test\n",
    "    inner_int = np.array([scipy.integrate.simpson(row,x=a_bin_centers) for row in zz*survey_sensitivity])\n",
    "    int1 = scipy.integrate.simpson(inner_int,x=m_bin_centers)\n",
    "    # this is a short cut to \"integrate\" I think, just occurrence rate * sensitivity * bin size\n",
    "    # int1 = np.sum(zz*survey_sensitivity)*(m_bins[1]-m_bins[0])*(a_bins[1]-a_bins[0])\n",
    "    return int1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does `N_exp_true` do?\n",
    "\n",
    "This function gives you a quick and simple way to guess how many planets your survey should find, using your model.  \n",
    "Instead of carefully checking how good your survey is at finding each type of planet, it just adds up the \u201cweights\u201d for all the planets you could have found, using your model and a simple multiplier.\n",
    "\n",
    "It\u2019s a rough estimate\u2014faster, but not as accurate as the more careful method.\n",
    "\n",
    "#### How does it work?\n",
    "\n",
    "- It uses your model to calculate a number for each possible planet (using their mass and distance).\n",
    "- It multiplies that number by each planet\u2019s \u201cweight\u201d (how likely it is to be found).\n",
    "- It multiplies by a simple constant (just a scaling factor).\n",
    "- It adds up all those numbers to get the total expected number of planets.\n",
    "\n",
    "> **Why do this?**\n",
    ">\n",
    "> This is useful if you want a quick answer or to test your code, but you should use the more careful method for real results.\n",
    "\n",
    "> **In short:** \n",
    ">\n",
    "> It\u2019s a shortcut to estimate how many planets you\u2019d expect to find, but it\u2019s not as trustworthy as the main method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def N_exp_true(theta,planet_sample,det_full):\n",
    "    \"\"\"\n",
    "    Estimate the expected detections by summing weights.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : dict or sequence\n",
    "        Occurrence rate parameters.\n",
    "    planet_sample : pandas.DataFrame\n",
    "        Planet sample.\n",
    "    det_full : pandas.DataFrame\n",
    "        Full detection catalog including weights.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Sum of weighted occurrence values.\n",
    "    \"\"\"\n",
    "    # \"integrate\" the expected number of detections by just summing weights. This is a too-accurate method,\n",
    "    # and the other should be preferred. Made for testing, and would need to be updated. \n",
    "    raww_col = 'final_weight'\n",
    "    int_value = 9\n",
    "    weights = mass_func_arr(theta,det_full)*det_full[raww_col]*int_value\n",
    "    #print(\"Nexp: \",np.sum(weights))                                                                                                                                                                                   \n",
    "    return np.sum(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does `ln_like` do?\n",
    "\n",
    "This function helps you figure out how well your model matches the planets you actually found in your survey.  \n",
    "It gives you a score (called the \u201clog likelihood\u201d)\u2014a bigger score means your model fits the data better.\n",
    "This score is the log-likelihood, based on Equation 6 from [Suzuki et al. (2016)](https://ui.adsabs.harvard.edu/abs/2016ApJ...833..145S/abstract).\n",
    "\n",
    "#### How does it work?\n",
    "\n",
    "- It uses your model to guess how many planets you should find (using `N_exp_real`).\n",
    "- It checks how well your model matches the planets you actually found, using their properties and how easy they were to detect.\n",
    "- It combines these numbers into a single score:  \n",
    "  - If your model predicts the right number and types of planets, the score is higher.\n",
    "  - If your model is way off, the score is lower.\n",
    "\n",
    "> **Why do this?**\n",
    ">\n",
    "> This score helps you compare different models and find the one that best matches your data.\n",
    "\n",
    "### What does `ln_like_neg` do?\n",
    "\n",
    "This is just a helper function for computers.  \n",
    "It takes the score from `ln_like` and flips it to a negative number.\n",
    "\n",
    "#### How does it work?\n",
    "\n",
    "- It runs `ln_like` and multiplies the answer by -1.\n",
    "\n",
    "> **Why do this?**\n",
    ">\n",
    "> Some computer tools like to find the smallest number (minimize), so this makes it easier for them to find the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ln_like(theta, planet_sample,survey_sensitivity):  \n",
    "    \"\"\"\n",
    "    Compute the log-likelihood for a set of parameters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : dict or sequence\n",
    "        Occurrence rate parameters.\n",
    "    planet_sample : pandas.DataFrame\n",
    "        Detected planet sample with sensitivity values.\n",
    "    survey_sensitivity : ndarray\n",
    "        Survey sensitivity map.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Natural logarithm of the likelihood.\n",
    "    \"\"\"\n",
    "    #logA, logB, logM_br, logM_slope, n, p, m = theta \n",
    "    # evaluate the mass function at all planets in the sample  \n",
    "    mass_func_vals = mass_func_arr(theta,planet_sample)\n",
    "    # determine the number of expected detections\n",
    "    # this should integrate over the survey sensitvity\n",
    "    N_exp_1 = N_exp_real(theta,planet_sample,survey_sensitivity)\n",
    "    # this version numerically determines the expected number of planets by just summing weights\n",
    "    #N_exp_2 = N_exp_true(theta,planet_sample,sensitivity_sum,parameters_to_fit,det_full)                                                                                                                               \n",
    "    #print('Nexp1, Nsamp :',N_exp_1, planet_sample.shape[0])\n",
    "    # log likelihood summation. Recall that the individual sensitivities are stored in the first column, \n",
    "    # which is done in data processing.\n",
    "    like_ = (-N_exp_1 + np.sum(np.log(mass_func_vals*planet_sample[\"sensitivity\"])))\n",
    "    \n",
    "    return like_\n",
    "\n",
    "# calculate the negative log likelihood for minimization\n",
    "def ln_like_neg(theta, planet_sample,survey_sensitivity):\n",
    "    \"\"\"\n",
    "    Negative log-likelihood for minimization routines.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : dict or sequence\n",
    "        Occurrence rate parameters.\n",
    "    planet_sample : pandas.DataFrame\n",
    "        Detected planet sample.\n",
    "    survey_sensitivity : ndarray\n",
    "        Survey sensitivity map.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Negative log-likelihood.\n",
    "    \"\"\"\n",
    "    return -1*ln_like(theta, planet_sample,survey_sensitivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does `ln_prior` do?\n",
    "\n",
    "This function checks if the numbers (parameters) you\u2019re using for your model are reasonable.  \n",
    "It helps make sure you don\u2019t try out crazy or impossible values when testing different models.\n",
    "\n",
    "#### How does it work?\n",
    "\n",
    "- It looks at each parameter in your model and checks if it\u2019s inside a \u201csafe\u201d range.\n",
    "- If all the parameters are in the allowed range, it returns 0 (which means \u201cokay!\u201d).\n",
    "- If any parameter is outside the allowed range, it returns a very bad score (\u2212infinity), which tells the computer to ignore this set of parameters.\n",
    "\n",
    "> **Why do this?**\n",
    ">\n",
    "> This keeps your model from trying out nonsense values and helps your results make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ln_prior(theta,paramters_to_fit=None):\n",
    "    \"\"\"\n",
    "    Prior probability for the model parameters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : sequence\n",
    "        Array of model parameters ``[logC, n, m]``.\n",
    "    paramters_to_fit : sequence, optional\n",
    "        Placeholder for parameter selection (unused).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        ``0.0`` if parameters are within allowed ranges, ``-np.inf`` otherwise.\n",
    "    \"\"\"\n",
    "    # prior ranges, right now just uniform priors. You will need to set these up for the \n",
    "    # variables being investigated, as well as suitable ranges\n",
    "    if not np.log10(0.01) < theta[0] < np.log10(10):\n",
    "        return -np.inf\n",
    "    if not -2 < theta[1] < 2:\n",
    "        return -np.inf\n",
    "    if not -2 < theta[2] < 2:\n",
    "        return -np.inf                                                                                                                                                                                       \n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does `ln_prob` do?\n",
    "\n",
    "This function checks how good your model is, using both the rules for what\u2019s allowed (the \u201cpriors\u201d) and how well it matches the data (the \u201clikelihood\u201d).  \n",
    "It gives you a final score for your model\u2014higher is better.\n",
    "\n",
    "#### How does it work?\n",
    "\n",
    "- It checks if your model\u2019s parameters are allowed (using `ln_prior`). If not, it gives a very bad score (\u2212infinity).\n",
    "- It checks how well your model matches the data (using `ln_like`). If something goes wrong, it also gives a bad score.\n",
    "- If everything is okay, it adds the two scores together to get the final answer.\n",
    "\n",
    "> **Why do this?**\n",
    ">\n",
    "> This is the main score used to compare different models. It helps you find the model that both makes sense and fits your data best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ln_prob(theta, planet_sample, det_full):\n",
    "    \"\"\"\n",
    "    Log-probability combining prior and likelihood.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : sequence\n",
    "        Model parameters.\n",
    "    planet_sample : pandas.DataFrame\n",
    "        Detected planet sample.\n",
    "    det_full : pandas.DataFrame\n",
    "        Survey sensitivity or detection grid.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Natural logarithm of the posterior probability.\n",
    "    \"\"\"\n",
    "    # calculate the prior value given theta  \n",
    "    ln_prior_ = ln_prior(theta) \n",
    "    # check that the prior did not go out of bounds; return negative infinity if it did\n",
    "    if not np.isfinite(ln_prior_):\n",
    "        return -np.inf\n",
    "    # calculate the likelihood\n",
    "    ln_like_ = ln_like(theta, planet_sample, det_full)\n",
    "\n",
    "    # if for some reason likelihood is nan, return negative infinity, shouldn't happen and can probably be removed                                                                                                                                                                       \n",
    "    if np.isnan(ln_like_):\n",
    "        return -np.inf\n",
    "    # multiply the prior and the lkikelihood\n",
    "    ln_prob_ = ln_prior_ + ln_like_\n",
    "    # return the probability, what emcee expects\n",
    "    return ln_prob_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-Up Functions\n",
    "\n",
    "---\n",
    "\n",
    "### What does `integrate_cassan` do?\n",
    "\n",
    "This function helps you figure out how many planets you\u2019d expect to find in a certain range of sizes (masses) and distances from their star, using a the [Cassan (2012)](https://ui.adsabs.harvard.edu/abs/2012Natur.481..167C/abstract) mass function.\n",
    "\n",
    "#### How does it work?\n",
    "\n",
    "- You give it a range for planet distance (`a_low` to `a_high`) and planet mass (`m_low` to `m_high`).\n",
    "- It uses a special formula to calculate how many planets should be in that range.\n",
    "- If the smallest mass is bigger than 5.2 Earth masses, it uses one formula.\n",
    "- If the range includes smaller planets, it splits the calculation and adds up both parts.\n",
    "\n",
    "> **Why do this?**\n",
    ">\n",
    "> This lets you estimate how many planets are in a certain part of your \u201cplanet map,\u201d using a model that matches what scientists have found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_cassan(a_low,a_high,m_low,m_high):\n",
    "    \"\"\"\n",
    "    Integrate the modified Cassan (2012) mass function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    a_low, a_high : float\n",
    "        Bounds on semi-major axis in au.\n",
    "    m_low, m_high : float\n",
    "        Bounds on planet mass in Earth masses.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Integral of the Cassan mass function over the given range.\n",
    "    \"\"\"\n",
    "    #earht masses                                                                                                                                                                                                      \n",
    "    a_int = (np.log(a_high)-np.log(a_low))/np.log(10)\n",
    "\n",
    "    if m_low > 5.2:\n",
    "        m_int = -4.06524/(m_high**(0.74)) + 4.06524/(m_low**(0.74))\n",
    "        return (a_int*m_int)\n",
    "    else:\n",
    "        m_int_low = 2*(np.log(5.2)-np.log(m_low))/np.log(10)\n",
    "        m_int_high = -4.06524/(m_high**(0.74)) + 4.06524/(5.2**(0.74))\n",
    "        return (m_int_low+m_int_high)*a_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does `integrate_loguniform` do?\n",
    "\n",
    "This function helps you figure out how many planets you\u2019d expect to find in a certain range, if planets are spread out evenly on a log scale (meaning, each \u201cstep\u201d in size or distance is just as likely as any other).\n",
    "\n",
    "#### How does it work?\n",
    "\n",
    "- You give it a range for planet distance (`a_low` to `a_high`) and planet mass (`m_low` to `m_high`).\n",
    "- It calculates the area of that range on a log scale (using logarithms).\n",
    "- The answer tells you how much of the \u201cplanet map\u201d that range covers, if everything is spread out evenly in log space.\n",
    "\n",
    "> **Why do this?**\n",
    ">\n",
    "> This is useful for comparing to a simple model where planets are equally likely at any size or distance (on a log scale), and for making fair comparisons between different parts of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_loguniform(a_low,a_high,m_low,m_high):\n",
    "    \"\"\"\n",
    "    Integrate a log-uniform distribution over the given range.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    a_low, a_high : float\n",
    "        Bounds on semi-major axis in au.\n",
    "    m_low, m_high : float\n",
    "        Bounds on planet mass in Earth masses.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Area in ``log10(a)``-``log10(m)`` space.\n",
    "    \"\"\"\n",
    "    return (np.log10(a_high)-np.log10(a_low)) * (np.log10(m_high)-np.log10(m_low))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does `calc_weights` do?\n",
    "\n",
    "This function helps you figure out how important each event (like each planet you found) is, by giving it a \u201cweight.\u201d  \n",
    "These weights help you count things fairly, taking into account how likely you were to find each event and how common they should be.\n",
    "\n",
    "#### How does it work?\n",
    "\n",
    "- It uses a math formula (an \u201cintegral\u201d) to figure out the total area you\u2019re looking at.\n",
    "- For each event, it calculates a special weight (using the `cassan` formula and other factors).\n",
    "- It multiplies the original weight for each event by these new factors to get the final weight.\n",
    "\n",
    "> **Why do this?**\n",
    ">\n",
    "> These weights make sure your results are fair and accurate, so you don\u2019t over-count or under-count certain types of events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_weights(df,event_rate_weight=1,int_func=integrate_loguniform,raww_col='final_weight'):\n",
    "    \"\"\"\n",
    "    Calculate final event weights using a survey model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Table of events.\n",
    "    event_rate_weight : float, optional\n",
    "        Overall scaling applied to all events.\n",
    "    int_func : callable, optional\n",
    "        Function used to integrate the assumed planet distribution.\n",
    "    raww_col : str, optional\n",
    "        Column in ``df`` containing the raw event weights.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ndarray\n",
    "        Array of updated weights.\n",
    "    \"\"\"\n",
    "    #get the cassan integral                                                                                                                                                                                           \n",
    "    int_value = int_func(a_low,a_high,a_low,a_high)\n",
    "    #for all events, calculate the cassan weight                                                                                                                                                                       \n",
    "    weight = cassan(data)\n",
    "    new_weights = df[raww_col]*cassan(data)*int_value*event_rate_weight\n",
    "    return new_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does `get_2dhist_values` do?\n",
    "\n",
    "This function helps you figure out, for each planet you found, how easy it was for your survey to detect it, based on its size and distance from its star.\n",
    "\n",
    "#### How does it work?\n",
    "\n",
    "- It looks at each planet\u2019s mass and distance.\n",
    "- It finds which \u201cbox\u201d (bin) on your survey\u2019s sensitivity map matches that planet.\n",
    "- It grabs the sensitivity value from that box\u2014this tells you how likely your survey was to find a planet like that.\n",
    "- It does this for every planet in your list and gives you back all the sensitivity values.\n",
    "\n",
    "> **Why do this?**\n",
    ">\n",
    "> Knowing the sensitivity for each planet helps you correct your results, so you don\u2019t miss out on planets that were hard to find or over-count ones that were easy to spot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_2dhist_values(grid,sample,a_bins,m_bins):\n",
    "    \"\"\"\n",
    "    Extract survey sensitivity for each planet in a sample.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    grid : ndarray\n",
    "        Two-dimensional survey sensitivity grid.\n",
    "    sample : pandas.DataFrame\n",
    "        Planet sample.\n",
    "    a_bins, m_bins : ndarray\n",
    "        Bin edges defining the grid in log space.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        Sensitivity value for each planet.\n",
    "    \"\"\"\n",
    "    mass_col = 'Planet_mass'\n",
    "    a_col = 'Planet_semimajoraxis'\n",
    "    a_bin_centers = ((a_bins[:-1]+a_bins[1:])/2)\n",
    "    m_bin_centers = a_bin_centers\n",
    "    mearth = 3.00374072e-6\n",
    "\n",
    "    sensitivities = []\n",
    "\n",
    "    for index, planet in sample.iterrows():\n",
    "        #print('here')\n",
    "        #print(planet,planet[mass_col])\n",
    "        m_ind = np.abs(m_bin_centers - np.log10(planet[mass_col]/mearth)).argmin()\n",
    "        a_ind = np.abs(a_bin_centers - np.log10(planet[a_col])).argmin()\n",
    "        #XXXensure these are correctly indexed                                                                                                                                                                         \n",
    "        sensitivities.append(grid[a_ind,m_ind])\n",
    "\n",
    "    return sensitivities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does `apply_cuts` do?\n",
    "\n",
    "This function helps you clean up your data by removing events (like planets) that don\u2019t meet certain rules.  \n",
    "It makes sure you only keep the events that are good enough for your analysis.\n",
    "\n",
    "#### How does it work?\n",
    "\n",
    "- It checks each event in your data to see if it passes several tests (called \u201ccuts\u201d):\n",
    "  - Is the event\u2019s signal strong enough?\n",
    "  - Does it look enough like a real planet and not something else?\n",
    "  - Is it in the right part of the sky and at the right time?\n",
    "  - Does it pass other special checks?\n",
    "- If an event fails any of these tests, it gets removed from your list.\n",
    "- You get back a new list with only the good events.\n",
    "\n",
    "> **Why do this?**\n",
    ">\n",
    "> This makes sure your results are based on reliable, high-quality data, so your conclusions are trustworthy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_cuts(dataFrame,flatchi2=500,chi2=160,u0max=3,t0inseason=True,noFS=False):\n",
    "    \"\"\"\n",
    "    Apply quality cuts to a catalog of detected events.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataFrame : pandas.DataFrame\n",
    "        Input catalog of events.\n",
    "    flatchi2 : float, optional\n",
    "        Minimum acceptable improvement over a flat fit.\n",
    "    chi2 : float, optional\n",
    "        Minimum acceptable improvement over the single-lens fit.\n",
    "    u0max : float, optional\n",
    "        Maximum allowed impact parameter.\n",
    "    t0inseason : bool, optional\n",
    "        Require the event to occur within a season.\n",
    "    noFS : bool, optional\n",
    "        Remove events flagged as finite-source.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Filtered catalog meeting all cuts.\n",
    "    \"\"\"\n",
    "    dat = dataFrame.copy()\n",
    "    # apply single lens chi2 cut - deafult is 500 above a flat light curve fit\n",
    "    dat = dat[dat['ObsGroup_0_flatchi2']>=flatchi2]\n",
    "    # apply binary lens cut - default is 160 above single lens fits\n",
    "    dat = dat[dat['ObsGroup_0_chi2']>=chi2]\n",
    "    # apply u0max cut- default is 3\n",
    "    dat = dat[np.abs(dat['u0lens1'])<u0max]\n",
    "    # apply inseasoncut\n",
    "    if t0inseason:\n",
    "        dat = dat[dat['ssn0']>0]\n",
    "    # apply no finite source cut\n",
    "    if noFS:\n",
    "        dat=dat[dat['ObsGroup_0_FiniteSourceflag']==0]\n",
    "    return dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "\n",
    "---\n",
    "\n",
    "### What are these variables and what are they for?\n",
    "\n",
    "These lines set up some basic numbers and grids that your analysis uses to organize and study planets:\n",
    "\n",
    "- `mearth`: The mass of Earth, used as a reference to compare other planet masses.\n",
    "- `a_low, a_high, m_low, m_high`: The lowest and highest values for planet distance from their star (semi-major axis, `a`) and planet mass (`m`). These set the boundaries for your study\u2014only planets within these ranges are considered.\n",
    "- `nbins`: The number of bins (sections) you want to split your mass and distance ranges into. More bins means finer detail, but can also make things noisier if you don\u2019t have a lot of data.\n",
    "- `m_bins`, `a_bins`: These are the edges of the bins for mass and distance, spaced evenly on a log scale. They define the grid you\u2019ll use to count and analyze planets.\n",
    "- `m_bin_centers`, `a_bin_centers`: The middle points of each bin, used for calculations and plotting.\n",
    "\n",
    "#### Why might you want to change these?\n",
    "\n",
    "- **Change the limits (`a_low`, `a_high`, `m_low`, `m_high`)** if you want to study a different range of planet sizes or distances.\n",
    "- **Change `nbins`** if you want more or less detail in your analysis grid. More bins = more detail, but you need enough data to fill them.\n",
    "- **Change `mearth`** only if you want to use a different reference mass (almost never needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definitions   \n",
    "# Mass of Earth \n",
    "mearth = 3.00374072e-6\n",
    "# limits for sma and mass bins, needs to match injected planet distribution\n",
    "a_low,a_high,m_low,m_high = 0.01,10.,0.01,10.\n",
    "# number of bins  \n",
    "nbins = 20\n",
    "# make bin edges for mass-sma grid   \n",
    "m_bins = np.linspace(np.log10(m_low),np.log10(m_high),nbins,endpoint=True)\n",
    "a_bins = np.linspace(np.log10(a_low),np.log10(a_high),nbins,endpoint=True)\n",
    "m_bin_centers = (m_bins[1:]+m_bins[:-1])/2  # tree?\n",
    "a_bin_centers = (a_bins[1:]+a_bins[:-1])/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Set-Up, Weights, and Visualization\n",
    "\n",
    "---\n",
    "\n",
    "### Loading Your Planet Data\n",
    "\n",
    "This cell loads all the planet events you want to analyze from a file.\n",
    "\n",
    "**How to use this cell:**\n",
    "- Make sure the `filename` variable points to the correct file you want to use. (It should be an HDF5 file with your planet data.)\n",
    "- The `column_names` list tells the code which columns (data fields) to read from the file.\n",
    "- When you run this cell, it reads the data and puts it into a table called `tot` that you can use for the rest of your analysis.\n",
    "\n",
    "> **Tip:**  \n",
    "> If you want to use a different data file, just change the `filename` path to point to your new file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in all events\n",
    "column_names = ['Lens_Mbol', 't0lens1', 'tcroin', 'u0lens1', 'ObsGroup_0_chi2', 'ObsGroup_0_flatchi2', 'ObsGroup_0_FiniteSourceflag', 'Lens_b', 'Lens_l', 'Field',\n",
    "                'Planet_semimajoraxis', 'Planet_mass', 'Planet_q',\n",
    "                'final_weight'\n",
    "               ]\n",
    "#filename = '/fs/project/gaudi.1/crisp/gulls/romanc7_hz_simple/analysis/romanc7_hz_simple.out.hdf5'\n",
    "filename = '/fs/project/gaudi.1/crisp/gulls/romanc7_hz_simple/analysis.real/romanc7_hz_simple.out.hdf5'\n",
    "tot = pd.read_hdf(filename, columns=column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Extra Information to Your Data (Insolation)\n",
    "\n",
    "This cell adds two new columns to your planet data to help with your analysis:\n",
    "\n",
    "- **`Planet_insolation`**:  \n",
    "  This calculates how much energy (light) each planet gets from its star, based on the star\u2019s brightness and the planet\u2019s distance.  \n",
    "  (Planets closer to bright stars get more energy; this is important for figuring out things like habitability.)\n",
    "\n",
    "- **`ssn0`**:  \n",
    "  This checks if the event (when the planet was found) happened during the \u201cseason\u201d when your survey was actually watching.  \n",
    "  (It uses the [`ssn`](#what-does-ssn-do) function to decide if the timing was right.)\n",
    "\n",
    "> **Why do this?**  \n",
    ">\n",
    "> Adding these columns gives you more information about each planet, so you can make better cuts, plots, or calculations later in your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform some calculations to append some columns that we will need\n",
    "# first is insolation\n",
    "tot['Planet_insolation'] = 10**(0.4*(4.74-tot['Lens_Mbol'])) * (tot['Planet_semimajoraxis'])**-2\n",
    "# add a column indicating if t0 occurred in season\n",
    "tot['ssn0'] = ssn(tot['t0lens1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the Survey Covering Factor and Final Weight Corrections\n",
    "\n",
    "This section adjusts the weights for each event (planet) in your data to account for how much of the sky your survey actually covered, and applies some other corrections.\n",
    "\n",
    "- **Covering Factor (`covfac`)**:  \n",
    "  Not every part of the sky was observed equally. The \u201ccovering factor\u201d tells you what fraction of each subfield (small patch of sky) was actually covered by your survey.  \n",
    "  For each event, its weight is multiplied by the covering factor for its field, so you don\u2019t over-count planets from areas you didn\u2019t fully observe.\n",
    "\n",
    "- **Final Weight Correction (`fwc`)**:  \n",
    "  The code creates a new column, `final_weight`, that includes this covering factor correction (and could include other corrections, like for false positives using [`FPF`](#what-does-fpf-do)).\n",
    "\n",
    "- **Survey Area Calculation**:  \n",
    "  The code also adds up the total area covered by your survey, using the covering factors and the size of each subfield. This is a \u201csanity check\u201d to make sure your survey area is what you expect.\n",
    "\n",
    "> **Why do this?**  \n",
    ">\n",
    "> These corrections make sure your results are accurate and fair, so you don\u2019t overestimate how common planets are by forgetting about parts of the sky you didn\u2019t actually observe.\n",
    "\n",
    "**Example outputs:**\n",
    "\n",
    "```\n",
    "0       l  -5.0\n",
    "1       b  -4.0\n",
    "2  covfac   0.0\n",
    "1.8931712961709124\n",
    "```\n",
    "\n",
    "> Note:\n",
    ">\n",
    "> That `scalefac` variable is supposed to normalize the event rates to something physically meaningful, like \"one planet per star per decade of mass,\" but Samson couldn't be bothered to explain how. For now, leaving it as scalefac = 1 is fine for validating the pipeline, but before you can get the absolute correct occurrence rate, you need to know what that number is supposed to be. It's Samson's homework, but ask Matt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Apply covering factor ####\n",
    "# This goes through and multiplies each event for the fraction of the subfield it was drawn from\n",
    "# that is covered by a given survey footprint.\n",
    "# Also apply corr for log-uniform planet injection\n",
    "tot['fwc'] = tot['final_weight']\n",
    "# Also, there is a false positive fraction (FPF) that can be accounted for from Penny 2019\n",
    "#dat['final_weight'] *= (1 - FPF(dat,mearth))\n",
    "covfac = pd.read_csv('/home/crisp.92/Programs/gulls/scripts/covfac/layout7f.approx.covfac',index_col='ID_src')\n",
    "# Running sum of the covering factor, adds together each unique covfac number and multiplies by\n",
    "# the subfield size to calculate the area of the survey footprint. Just a sanity check. \n",
    "cov_sum = 0\n",
    "# for each unique field of events\n",
    "for field in np.unique(tot['Field']):\n",
    "    # generate a mask for events that were drawn from that field\n",
    "    mask = tot['Field'] == field\n",
    "    # Essentially just renormalizes the event rates to be \"one per star per mass per SMA bin\"\n",
    "    scalefac = 1  # !!!\n",
    "    # This creates a new final weight corrected (fwc) column\n",
    "    tot.loc[mask,'final_weight'] = scalefac * covfac.loc[field]['covfac'] * tot.loc[mask]['final_weight']\n",
    "    # update running covfac sum\n",
    "    cov_sum += covfac.loc[field]['covfac']*.2*.2\n",
    "    # print number of events, the field number, and the covfac value\n",
    "    #print(tot.loc[mask,'final_weight'].shape,field,covfac.loc[field]['covfac'])\n",
    "# print survey area from covfac calc\n",
    "print(cov_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Events with Detection Cuts\n",
    "\n",
    "This section filters your data to create two new tables of events, based on how strict the detection rules (\u201ccuts\u201d) are:\n",
    "\n",
    "- **`dat`**:  \n",
    "  This table includes all events that pass the basic \u201cin season\u201d and signal strength checks, but with a very loose (almost no) cut on the \u201cchi2\u201d value.  \n",
    "  (It uses the [`apply_cuts`](#what-does-apply_cuts-do) function.)\n",
    "\n",
    "- **`det`**:  \n",
    "  This table includes only the events that pass both the \u201cin season\u201d check and a stricter \u201cchi2\u201d cut, meaning they are more likely to be real planet detections.\n",
    "\n",
    "> **Why do this?**  \n",
    ">\n",
    "> By making two filtered lists, you can compare all possible events to the ones that are most likely real planets, which helps you understand your survey\u2019s performance and the reliability of your detections.\n",
    "\n",
    "#### A Note on Filtering\n",
    "\n",
    "As per Ali's notes, this step creates our two essential dataframes:\n",
    "* **`dat`**: Represents all events that could have been detected (similar to `N_dat`), passing a basic `flatchi2 > 500` cut.\n",
    "* **`det`**: Represents the events that were *actually* detected (our `N_det`), passing the stricter `chi2 > 160` cut.\n",
    "\n",
    "These cuts are essential for correctly calculating the detection efficiency later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter tot for only events with t0 in season\n",
    "# dat is essentially single lens detected events with t0inSeason\n",
    "dat = apply_cuts(tot,flatchi2=500, chi2=-1e9)\n",
    "# det is detected planets with t0inSeason\n",
    "det = apply_cuts(tot,flatchi2=500, chi2=160)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Survey Field Extents\n",
    "\n",
    "This section makes a quick plot to show where your events are on the sky.\n",
    "\n",
    "- It prints the minimum and maximum values of `Lens_b` (latitude) and `Lens_l` (longitude) for your events, so you can see the range your survey covers.\n",
    "- It adjusts any longitude values greater than 300 by subtracting 360, to keep all longitudes in a standard range (useful for plotting).\n",
    "- It then makes a scatter plot of longitude vs. latitude for all your events, so you can visualize the area of the sky your survey looked at.\n",
    "\n",
    "> **Why do this?**  \n",
    ">\n",
    "> This plot gives you a quick check that your data covers the sky as expected, and helps you spot any weird outliers or mistakes in the event positions.\n",
    "\n",
    "**Example Output:**  \n",
    "\n",
    "```\n",
    "-2.2057731 -0.39435861\n",
    "5.2688221e-09 360.0\n",
    "```\n",
    "\n",
    "Below is an example of what this plot should look like:\n",
    "\n",
    "![Example survey field plot](ExampleFigures/Figure1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just make a plot of field extents more less, not important\n",
    "print(np.min(dat['Lens_b']),np.max(dat['Lens_b']))\n",
    "print(np.min(dat['Lens_l']),np.max(dat['Lens_l']))\n",
    "dat.loc[dat['Lens_l']>300,'Lens_l'] = dat[dat['Lens_l']>300]['Lens_l'] - 360\n",
    "plt.plot(dat['Lens_l'],dat['Lens_b'],'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Distribution of Planet Distances\n",
    "\n",
    "This section makes a histogram (bar chart) showing how many planets you have at different distances from their stars (measured as the semi-major axis, or SMA).\n",
    "\n",
    "- It takes the logarithm of each planet\u2019s distance to spread out the values more evenly (since distances can vary a lot).\n",
    "- It counts how many planets fall into each distance bin and makes a plot.\n",
    "\n",
    "> **Why do this?** \n",
    "> \n",
    "> This plot helps you see where most of your planets are\u2014are they mostly close to their stars, or far away? It\u2019s a quick way to understand the range and distribution of your data.\n",
    "\n",
    "**Example Output:**  \n",
    "Below is an example of what this plot should look like:\n",
    "\n",
    "![Example SMA distribution plot](ExampleFigures/Figure2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of SMA\n",
    "plt.hist(np.log10(tot['Planet_semimajoraxis']),bins=200)#,weights=det['fwc'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Distribution of Event Times\n",
    "\n",
    "This section makes two histograms showing when your events happened, using two different time measurements (`t0lens1` and `tcroin`).\n",
    "\n",
    "- It counts how many events occurred at each time, using the final weights (`fwc`) to make the counts fair.\n",
    "- Both histograms are shown together, so you can compare the timing of the two types of events.\n",
    "\n",
    "> **Why do this?** \n",
    "> \n",
    "> This plot helps you see if your events are spread out evenly over time, or if there are certain periods when more events were detected. It can also help you spot any gaps or patterns in your data collection.\n",
    "\n",
    "**Example Output:**  \n",
    "Below is an example of what this plot should look like:\n",
    "\n",
    "![Example event time distribution plot](ExampleFigures/Figure3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(tot['t0lens1'],range=[0,2010],bins=200,weights=tot['fwc'])\n",
    "plt.hist(tot['tcroin'],range=[0,2010],bins=200,weights=tot['fwc'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Distribution of Impact Parameters\n",
    "\n",
    "This section makes a histogram showing the distribution of `u0lens1`, which is the impact parameter for each event (a measure of how close the event was to the center of the lensing effect).\n",
    "\n",
    "- It counts how many events have each value of `u0lens1`, using the final weights (`fwc`) to make the counts fair.\n",
    "- The range is set from -5 to 5, and the data is split into 200 bins.\n",
    "\n",
    "> **Why do this?**  \n",
    ">\n",
    "> This plot helps you see if your events are mostly close to the center (small `u0lens1`) or more spread out. It\u2019s useful for checking if your data matches what you\u2019d expect from your survey and models.\n",
    "\n",
    "**Example Output:**  \n",
    "Below is an example of what this plot should look like:\n",
    "\n",
    "![Example impact parameter distribution plot](ExampleFigures/Figure4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(tot['u0lens1'],range=[-5,5],bins=200,weights=tot['fwc'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the Total Number of Corrected Events\n",
    "\n",
    "This section calculates the total number of events in your filtered data, after applying all the corrections and weights.\n",
    "\n",
    "- It adds up the `final_weight` for all events in your filtered table (`dat`).\n",
    "- The result is printed out as the total \u201ccorrected\u201d number of events.\n",
    "\n",
    "> **Why do this?**  \n",
    ">\n",
    "> This gives you a fair count of how many events you really have, taking into account all the adjustments for survey coverage, detection efficiency, and other corrections.\n",
    "\n",
    "**Example Output:**  \n",
    "```\n",
    "311680.162493718675\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of events\n",
    "corrected_total_events = np.sum(dat['final_weight'])\n",
    "print(corrected_total_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection Efficiency\n",
    "\n",
    "### Building 2D Histograms for Mass and Distance\n",
    "\n",
    "This section creates two 2D histograms (tables) that count how many events you have for each combination of planet mass and distance from their star (semi-major axis).\n",
    "\n",
    "- **`dat_eff`**:  \n",
    "  Counts all filtered events, using their weighted values, and sorts them into bins based on the logarithm of their distance and mass.\n",
    "\n",
    "- **`det_eff`**:  \n",
    "  Does the same thing, but only for the events that passed the stricter detection cuts (likely real planets).\n",
    "\n",
    "> **Why do this?**  \n",
    ">\n",
    "> These 2D histograms let you see how your events are distributed across both mass and distance at the same time.  \n",
    "They are useful for making heatmaps or for further analysis of how detection efficiency changes with planet properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build histograms for mass-SMA\n",
    "dat_eff = np.histogram2d(np.log10(dat['Planet_semimajoraxis']),np.log10(dat['Planet_mass']/mearth),bins=[a_bins,m_bins],\n",
    "                          weights=dat['fwc'])     \n",
    "det_eff = np.histogram2d(np.log10(det['Planet_semimajoraxis']),np.log10(det['Planet_mass']/mearth),bins=[a_bins,m_bins],\n",
    "                          weights=det['fwc'])     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Detection Efficiency Map\n",
    "\n",
    "This section calculates and visualizes how good your survey is at detecting planets, depending on their mass and distance from their star (semi-major axis).\n",
    "\n",
    "- It computes the detection efficiency (`det_efficiency`) for each bin by dividing the number of detected events by the total number of possible events.\n",
    "- It then calculates the **Survey Sensitivity (`S`)**, defined as `S = det_efficiency * N_events` [(Suzuki et al. 2016)](https://ui.adsabs.harvard.edu/abs/2016ApJ...833..145S/abstract). This tells you how many planets you would have detected in that bin if every single star had a planet.\n",
    "- It multiplies this efficiency by the total number of corrected events to get the survey sensitivity.\n",
    "- It then makes a heatmap plot, where color shows how easy or hard it is to detect planets in each part of the mass\u2013distance grid.\n",
    "\n",
    "> **Why do this?**  \n",
    ">\n",
    "> This plot helps you see where your survey is most sensitive (where you\u2019re most likely to find planets) and where it\u2019s less sensitive. It\u2019s a key tool for understanding and correcting for biases in your results.\n",
    "\n",
    "**Example Output:**  \n",
    "Below is an example of what this plot should look like:\n",
    "\n",
    "![Example detection efficiency map](ExampleFigures/Figure5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the detection sensitivity in terms of mass-SMA. \n",
    "det_efficiency = det_eff[0]/dat_eff[0] \n",
    "survey_sensitivity = det_efficiency * corrected_total_events\n",
    "if True:        \n",
    "    fig,ax = plt.subplots()\n",
    "    phist = np.log10(det_efficiency).T\n",
    "    #phist = np.log10(det_eff).T\n",
    "    phist = np.clip(phist,-4,None)\n",
    "    im = ax.imshow(phist,origin='lower',extent=[np.log10(a_low),np.log10(a_high),np.log10(m_low),np.log10(m_high)],vmin=-4,vmax=-1,cmap='cividis')\n",
    "    #ax.scatter(np.log10(planet_sample['Planet_semimajoraxis']),np.log10(planet_sample['Planet_mass']/mearth),c=np.log10(planet_sample['sensitivity']/corrected_total_events),vmin=-4,vmax=-1,cmap='cividis')\n",
    "    ax.set_xlabel(r'Semi-Major Axis [$\\log(a/a_\\oplus)$]',fontsize=16)\n",
    "    ax.set_ylabel(r'Planet mass [$\\log(M_p/M_\\oplus)$]',fontsize=16)\n",
    "    #plt.colorbar(im,label=r'$\\log (N_{det})$ if each lens has one planet at $(M,a)$')\n",
    "    cb = plt.colorbar(im,label=r'$\\log ({Detection Efficiency})$')\n",
    "    cb.set_label(label=r'$\\log ({Detection Efficiency})$', fontsize=16)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "    cb.ax.tick_params(labelsize=12)\n",
    "    plt.savefig('ma_map.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Detection Efficiency Map in Mass\u2013Insolation Space\n",
    "\n",
    "This section calculates and visualizes how good your survey is at detecting planets, depending on their mass and the amount of energy (insolation) they receive from their star.\n",
    "\n",
    "- It creates 2D histograms for both all events and detected events, sorted by planet mass and insolation.\n",
    "- It computes the detection efficiency for each combination of mass and insolation by dividing the number of detected events by the total number of possible events in each bin.\n",
    "- It multiplies this efficiency by the total number of corrected events to get the survey sensitivity.\n",
    "- It then makes a heatmap plot, where color shows how easy or hard it is to detect planets in each part of the mass\u2013insolation grid.\n",
    "\n",
    "> **Why do this?**  \n",
    ">\n",
    "> This plot helps you see where your survey is most sensitive to finding planets, not just by their mass and distance, but also by how much energy they get from their star\u2014a key factor for habitability studies.\n",
    "\n",
    "**Example Output:**  \n",
    "Below is an example of what this plot should look like:\n",
    "\n",
    "![Example detection efficiency in mass\u2013insolation space](ExampleFigures/Figure6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build histograms for mass-insolation\n",
    "s_low, s_high = 0.01, 10\n",
    "s_bins = np.linspace(np.log10(s_low),np.log10(s_high),nbins,endpoint=True)\n",
    "dat_eff_insol = np.histogram2d(np.log10(dat['Planet_insolation']),np.log10(dat['Planet_mass']/mearth),bins=[s_bins,m_bins],weights=dat['final_weight'])     \n",
    "det_eff_insol = np.histogram2d(np.log10(det['Planet_insolation']),np.log10(det['Planet_mass']/mearth),bins=[s_bins,m_bins],weights=det['final_weight'])#/det_uniform[:,50]*3)#/det_uniform[:,50])\n",
    "\n",
    "# detection efficiency in mass-insolation space     \n",
    "efficiency_insol = det_eff_insol[0]/dat_eff_insol[0]\n",
    "# survey sensitivity in mass-SMA space   \n",
    "s_survey_sensitivity = efficiency_insol * corrected_total_events\n",
    "\n",
    "s_bin_centers = (s_bins[1:]+s_bins[:-1])/2\n",
    "m_bin_centers = (m_bins[1:]+m_bins[:-1])/2\n",
    "test = 1#occ_arr(true_parameters,a_bin_centers,m_bin_centers)\n",
    "fig,ax = plt.subplots()\n",
    "phist = np.log10(efficiency_insol).T#[:,::-1]\n",
    "phist = np.clip(phist,-5,None)\n",
    "im = ax.imshow(phist,origin='lower',extent=[np.log10(s_high),np.log10(s_low),np.log10(m_low),np.log10(m_high)],vmin=-4,vmax=-1,cmap='cividis')\n",
    "#ax.scatter(np.log10(planet_sample[:,43]),np.log10(planet_sample[:,42]/mearth),c=np.log10(planet_sample[:,0]),vmin=-4,vmax=2,cmap='cividis')  \n",
    "ax.set_xlabel(r'Insolation [$\\log(S/S_\\oplus)$]',fontsize=16)\n",
    "ax.set_ylabel(r'Planet mass [$\\log(M_p/M_\\oplus)$]',fontsize=16)\n",
    "cb = plt.colorbar(im,label=r'$\\log ({Detection Efficiency})$')\n",
    "cb.set_label(label=r'$\\log ({Detection Efficiency})$', fontsize=16)\n",
    "ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "cb.ax.tick_params(labelsize=12)\n",
    "plt.savefig('ms_map.pdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Detected Planets by Mass\n",
    "\n",
    "This section creates a histogram that counts how many detected planets you have in each mass bin.\n",
    "\n",
    "- It takes the logarithm of each detected planet\u2019s mass (relative to Earth) and sorts them into bins.\n",
    "- It uses the final weights (`fwc`) to make the counts fair.\n",
    "- The result is an array of counts\u2014one for each mass bin.\n",
    "\n",
    "> **Why do this?**  \n",
    ">\n",
    "> This gives you a quick summary of how your detected planets are distributed by mass, which is useful for understanding your sample and for further statistical analysis.\n",
    "\n",
    "**Example Output:**  \n",
    "```\n",
    "array([ 0.05476764, 0.27029432, 0.10063701, 0.11163519, 0.15077024,\n",
    "0.33513537, 0.7068122 , 0.83286857, 1.43670399, 1.84264488,\n",
    "3.35712462, 4.2943674 , 5.78544942, 7.77331711, 11.30759032,\n",
    "13.99050355, 19.20021137, 23.36225181, 30.17258706])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det_hist = np.histogram(np.log10(det['Planet_mass']/mearth),bins=m_bins,weights=det['fwc'])\n",
    "det_hist[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Detected Planet Mass Distribution\n",
    "\n",
    "This section makes a line plot showing how many detected planets you have in each mass bin.\n",
    "\n",
    "- It plots the center of each mass bin on the x-axis (in log scale, relative to Earth\u2019s mass).\n",
    "- It plots the logarithm of the number of detected planets in each bin on the y-axis.\n",
    "- The plot includes grid lines for easier reading.\n",
    "\n",
    "> **Why do this?**  \n",
    ">\n",
    "> This plot helps you quickly see which planet masses are most common in your detected sample, and whether there are trends or gaps in your data.\n",
    "\n",
    "**Example Output:**  \n",
    "Below is an example of what this plot should look like:\n",
    "\n",
    "![Example detected planet mass distribution plot](ExampleFigures/Figure7.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(m_bin_centers,np.log10(det_hist[0]),label='Detections')\n",
    "plt.xlabel(r\"$\\log(M_p/M_\\oplus)$\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Survey Sensitivity\n",
    "\n",
    "### Plotting Survey Sensitivity by Planet Mass\n",
    "\n",
    "This section visualizes how sensitive your survey is to detecting planets of different masses.\n",
    "\n",
    "- It integrates the survey sensitivity over all distances for each mass bin, showing how likely you are to detect a planet of each mass.\n",
    "- It plots a line for each mass bin, colored sequentially, to show the sensitivity pattern.\n",
    "- The thick black line shows the total sensitivity summed over all bins.\n",
    "- The y-axis is on a logarithmic scale to show a wide range of values.\n",
    "\n",
    "> **Why do this?**  \n",
    ">\n",
    "> This plot helps you see which planet masses your survey is best at detecting, and where it might be missing planets due to lower sensitivity.\n",
    "\n",
    "**Example Output:**  \n",
    "Below is an example of what this plot should look like:\n",
    "\n",
    "![Example survey sensitivity by mass plot](ExampleFigures/Figure8.png)\n",
    "\n",
    "```\n",
    "Total Events  31680.162493718675\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.get_cmap('viridis')\n",
    "mass_sens = scipy.integrate.simpson(survey_sensitivity,dx=a_bins[1]-a_bins[0],axis=0)\n",
    "tot_sens = scipy.integrate.simpson(mass_sens,dx=a_bins[1]-a_bins[0],axis=0)\n",
    "print(tot_sens)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot lines with sequential colors\n",
    "for i, y in enumerate(m_bin_centers):\n",
    "    ax.plot(m_bin_centers, survey_sensitivity[i,:], color=cmap(i / len(m_bin_centers)))\n",
    "    \n",
    "ax.plot(m_bin_centers,mass_sens,linewidth=5)\n",
    "\n",
    "plt.ylabel('Ndet per mass bin')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('log(m)')\n",
    "plt.show()\n",
    "print('Total Events ', corrected_total_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE Occurance Rate Fit Preparation\n",
    "\n",
    "---\n",
    "\n",
    "### Setting Up True Model Parameters for Testing\n",
    "\n",
    "This section defines the \u201ctrue\u201d parameters for your planet occurrence rate model.  \n",
    "These are the values you\u2019ll use as a reference or for testing your fitting code.\n",
    "\n",
    "- It makes copies of your filtered data (`dat_real` and `det_real`) to keep the originals safe.\n",
    "- It sets up two sets of model parameters:\n",
    "  - **Full model:** Uses a broken power law with different slopes and break points for planet mass.\n",
    "  - **Simplified model:** Uses a single power law for planet mass and distance.\n",
    "- The parameters are stored in dictionaries (`true_parameters` and `true_params`) for easy use in later calculations or fitting routines.\n",
    "\n",
    "> **Why do this?**  \n",
    ">\n",
    "> Defining these \u201ctrue\u201d parameters lets you test your analysis pipeline, check if your code can recover known values, and compare different models.  \n",
    "> It\u2019s a key step for validating your results and understanding how your model works.\n",
    "\n",
    "#### Note on the Model Being Fit\n",
    "\n",
    "Right now, we're fitting a simple power law for mass (`n`) and semi-major axis (`m`):\n",
    "\n",
    "$$ \\frac{d^2N}{d\\log(m_p) d\\log(a)} = C \\cdot m_p^n \\cdot a^m $$\n",
    "\n",
    "The ultimate goal is to implement and fit a more complex, scientifically-motivated model, like the modified Cassan model from [Penny (2019)](https://ui.adsabs.harvard.edu/abs/2019ApJS..241....3P/abstract), which uses a broken power law for the mass distribution. For now, this simpler model is good for testing the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make copies just in case   \n",
    "dat_real = dat.copy()\n",
    "det_real = det.copy()\n",
    "\n",
    "#for full functional fitting of broken power law with varying break point in mass\n",
    "#define injected occurrence rate parameters    \n",
    "logC1_true = np.log10(2)\n",
    "logMS1_true = 1\n",
    "n1_true = 0\n",
    "#logMbreak_true = np.log10(5.2)     \n",
    "Mbreak_true = (5.2)\n",
    "logC2_true = np.log10(2)\n",
    "logMS2_true = np.log10(95)\n",
    "n2_true = -0.73\n",
    "m_true = 0\n",
    "\n",
    "#set up as a dictionary    \n",
    "true_parameters = {'logC1':logC1_true,\n",
    "                   'logMS1':logMS1_true,\n",
    "                   'n1':n1_true,\n",
    "                   'logMbreak':np.log10(Mbreak_true),\n",
    "                   'logC2':logC2_true,\n",
    "                   'logMS2':logMS2_true,\n",
    "                   'n2':n2_true,\n",
    "                   'm':m_true}\n",
    "\n",
    "#simplified version \n",
    "n_true = -1\n",
    "m_true =0\n",
    "logC_true = np.log10(2)    \n",
    "\n",
    "#set up as a dictionary    \n",
    "true_params = {'logC':logC_true,\n",
    "               'n':n_true,\n",
    "               'm':m_true}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawing a Sample of Detected Planets for Analysis\n",
    "\n",
    "This section prepares a sample of detected planets for further analysis, using your model and survey corrections.\n",
    "\n",
    "- It calculates a new weight (`fwc`) for each detected planet, combining the final weight, a normalization factor, and the value from your occurrence rate model (using [`mass_func_arr`](#what-does-mass_func_arr-do)).\n",
    "- It prints the expected total number of detections based on these weights.\n",
    "- It draws a random sample of planets from your detected events, using the new weights to make the selection fair (using [`draw_sample`](#what-does-draw_sample-do)).\n",
    "- It copies the sample for safety and then calculates the detection sensitivity for each planet in the sample (using [`get_2dhist_values`](#what-does-get_2dhist_values-do)).\n",
    "- It prints the actual number of planets in the sample.\n",
    "\n",
    "> **Why do this?**  \n",
    ">\n",
    "> This process gives you a realistic set of detected planets, accounting for all the corrections and your model, so you can use them for statistical analysis or further modeling.\n",
    "\n",
    "**Example Output:**  \n",
    "\n",
    "```\n",
    "Expected total detections: 953.78942559785\n",
    "Actual sample size: 968\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### draw planet sample    \n",
    "#using cassan weights, currently in -2 (or -8 for uctc altered file)      \n",
    "# new: where is -2 coming from???     \n",
    "#new weights = raww * 3 * 3 * (occurrence rate value) \n",
    "#the 3's come from normalizing to the injected distribution of planets   \n",
    "det['fwc'] = 9*det['final_weight']*mass_func_arr(true_params,det)\n",
    "print(\"Expected total detections:\",np.sum(det['fwc']))\n",
    "#draw a sample          \n",
    "planet_temp = draw_sample(det,weight_col='fwc')\n",
    "#copy just in case? Not sure  \n",
    "planet_sample = planet_temp.copy()\n",
    "#retrieve the individual senstivity values for the sample of planets      \n",
    "planet_sample['sensitivity'] =  get_2dhist_values(survey_sensitivity,planet_sample,a_bins,m_bins)\n",
    "print(\"Actual sample size:\", planet_sample.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the Normalization of Detection Weights\n",
    "\n",
    "This section checks that your detection weights and calculations are consistent.\n",
    "\n",
    "- It calculates the ratio of the sum of the new weights (`fwc`) to the sum of the original final weights for detected planets.\n",
    "- It prints the sum of the expected detections using two different methods, to make sure they match:\n",
    "  - Directly multiplying the final weights by the occurrence rate model and normalization factor.\n",
    "  - Summing the already-calculated `fwc` values.\n",
    "\n",
    "> **Why do this?**  \n",
    ">\n",
    "> This is a quick consistency check to make sure your weights and calculations are correct. If the printed numbers match, you know your normalization is working as expected.\n",
    "\n",
    "**Example Output:**  \n",
    "\n",
    "```\n",
    "953.78942559785\n",
    "953.78942559785\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(det['fwc'])/np.sum(det['final_weight'])\n",
    "print(np.sum(9*det['final_weight']*mass_func_arr(true_params,det)))\n",
    "print(np.sum(det['fwc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Modelling\" the Occurance Rates\n",
    "\n",
    "---\n",
    "\n",
    "### Fitting the Occurrence Rate Model to Your Sample\n",
    "\n",
    "This section fits your planet occurrence rate model to the sample of detected planets, using maximum likelihood estimation (MLE).\n",
    "\n",
    "- It sets up the model parameters (like normalization, mass slope, and distance slope) and their allowed ranges.\n",
    "- It uses the [`lmfit`](https://lmfit.github.io/lmfit-py/) library to find the best-fit parameters that maximize the likelihood (using [`ln_like_neg`](#what-does-ln_like_neg-do)).\n",
    "- After fitting, it calculates the expected number of detected planets using the best-fit model and the survey sensitivity.\n",
    "- It integrates the occurrence rates over the grid to get the total expected number of detections, and prints the result.\n",
    "\n",
    "> **Why do this?**  \n",
    ">\n",
    "> This step finds the model that best matches your data, allowing you to estimate how common different types of planets are, given your survey\u2019s sensitivity.\n",
    "\n",
    "**Example Output:**  \n",
    "\n",
    "```\n",
    "N_exp from fit: 968.0690398100909 968.0690398100909\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fitting the sample  \n",
    "params = lmfit.Parameters()\n",
    "n_value = 0\n",
    "\n",
    "#some more or less random values at this point. Boundaries are fairly lenient.\n",
    "# boolean for each paramater determines if that parameter is varied \n",
    "params.add_many(('logC',np.log10(1),True,0,3),\n",
    "                ('n',n_value,True,-5,5),\n",
    "                ('m',0,True,-5,5))\n",
    "\n",
    "# run the MLE fit \n",
    "#survey_sensitivity = np.nan_to_num(survey_sensitivity,nan=0)\n",
    "#res = lmfit.minimize(ln_like_neg, params, args=(planet_sample,det_real),method='Nelder')#,bounds=bounds)#,callback=callbackF) \n",
    "#res = lmfit.minimize(ln_like_neg, params, args=(planet_sample,survey_sensitivity),method='Nelder')#,bounds=bounds)#,callback=callbackF) \n",
    "res = lmfit.minimize(ln_like_neg, params, args=(planet_sample,survey_sensitivity),method='Nelder')#,bounds=bounds)#,callback=callbackF) \n",
    "\n",
    "# essentially formatting   \n",
    "occ_arr_res = occ_arr(res.params,a_bin_centers,m_bin_centers)\n",
    "det_yield = (occ_arr_res*survey_sensitivity)\n",
    "#print(occ_arr_res)\n",
    "# integrate the occurrence rates  \n",
    "#int1 = scipy.integrate.simpson([scipy.integrate.simpson(zz_x,x=a_bin_centers) for zz_x in occ_arr_res.T*survey_sensitivity],x=m_bin_centers)\n",
    "inner_int = np.array([scipy.integrate.simpson(row,x=a_bin_centers) for row in det_yield])\n",
    "outer_int = scipy.integrate.simpson(inner_int,x=m_bin_centers)\n",
    "int1 = N_exp_real(res.params, planet_sample, survey_sensitivity)\n",
    "print(\"N_exp from fit: \", outer_int, int1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing the Best-Fit Model Parameters\n",
    "\n",
    "This line prints out the values of your best-fit model parameters after fitting:\n",
    "\n",
    "- `10**res.params['logC']`: The normalization constant for your occurrence rate model (converted from log scale to normal scale).\n",
    "- `res.params['n'].value`: The slope of the occurrence rate with respect to planet mass.\n",
    "- `res.params['m'].value`: The slope of the occurrence rate with respect to semi-major axis (distance).\n",
    "\n",
    "> **Why do this?**  \n",
    ">\n",
    "> Printing these values tells you the actual numbers your model found to best match your data.  \n",
    "You can use these to interpret your results, compare to other studies, or use them in further calculations.\n",
    "\n",
    "**Example Output:**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(10**res.params['logC'], res.params['n'].value, res.params['m'].value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Occurance Rate Fitting Results\n",
    "\n",
    "---\n",
    "\n",
    "### Comparing Observed and Model Planet Mass Distributions\n",
    "\n",
    "This section creates a plot that compares the observed distribution of planet masses to the predictions from your occurrence rate model.\n",
    "\n",
    "- It shows several lines:\n",
    "  - The raw (uncorrected) counts of planets per mass bin.\n",
    "  - The corrected counts, accounting for survey sensitivity and other factors.\n",
    "  - The \u201ctrue\u201d occurrence rate used for testing (if available).\n",
    "  - The best-fit occurrence rate from your model.\n",
    "- The y-axis is on a logarithmic scale to show a wide range of values.\n",
    "- The plot includes a legend to explain each line.\n",
    "\n",
    "> **Why do this?**  \n",
    ">\n",
    "> This plot lets you see how well your model matches the data, after all corrections. It\u2019s a key check for the quality of your fit and the reliability of your results.\n",
    "\n",
    "**Example Output:**  \n",
    "Below is an example of what this plot should look like:\n",
    "\n",
    "![Example planet mass distribution comparison plot](ExampleFigures/Figure9.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# survey_sensitivity integrated over semi-major axis (a) \n",
    "mass_sens = scipy.integrate.simpson(survey_sensitivity,dx=a_bins[1]-a_bins[0],axis=0)\n",
    "#mass_sens = scipy.integrate.simpson(det_efficiency.T,dx=a_bins[1]-a_bins[0],axis=1)\n",
    "\n",
    "# plot the results of the LME     \n",
    "m_bin_centers = (m_bins[:-1]+m_bins[1:])/2\n",
    "# build mass histograms, relevant vars  \n",
    "hist,bins = np.histogram(np.log10(planet_sample['Planet_mass']/mearth),bins=m_bins)\n",
    "hist_true,bins = np.histogram(np.log10(det['Planet_mass']/mearth),bins=m_bins,weights=det['fwc'])\n",
    "\n",
    "# histograms of full sim results  \n",
    "# THIS LINE WAS RECENTLY HAD LAST COMMENT ADDED \n",
    "massw,bins = np.histogram(np.log10(dat['Planet_mass']/mearth),bins=m_bins,weights=dat['fwc'])\n",
    "massd = np.histogram(np.log10(det['Planet_mass']/mearth),bins=m_bins,weights=det['fwc'])[0]\n",
    "\n",
    "# I don't think this is needed\n",
    "sma_sens = np.histogram(np.log10(det['Planet_semimajoraxis']),bins=a_bins,weights=det['fwc'])[0]/np.histogram(np.log10(dat['Planet_semimajoraxis']),bins=a_bins,weights=dat['fwc'])[0]\n",
    "m_points = np.linspace(-2,1,1000)  \n",
    "\n",
    "# plot mass\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "# could be needed for normalization, need to actually plug in real integral form\n",
    "sma_int = 1 #(a_high**res.params['m'] - a_low**res.params['m']) / res.params['m'] / np.log(10) / 3\n",
    "print(sma_int)\n",
    "\n",
    "\n",
    "# plot uncorrected counts (units are counts per bin width)   \n",
    "ax.semilogy(m_bin_centers, hist/(m_bins[1]-m_bins[0]),'C0--',label='Raw Sample/bin')#*(m_bins[1]-m_bins[0]))                                                                                                  \n",
    "# plot true counts per bin width     \n",
    "ax.semilogy(m_bin_centers,( hist_true/(m_bins[1]-m_bins[0])),'C6--',label='Raw Counts/bin')#*(m_bins[1]-m_bins[0]))\n",
    "# calculate the corrected corrected observed counts  \n",
    "corr = hist  / mass_sens / sma_int / (m_bins[1]-m_bins[0]) #/corr_fac                                                                                                                                               \n",
    "# calculate the true injected mass function  \n",
    "true_x = mass_func_arr(true_params,m_bin_centers)\n",
    "# calculate the corrected true counts                                                                                                                                                                     \n",
    "corr1 = hist_true / mass_sens / sma_int / (m_bins[1]-m_bins[0]) #/ corr_fac                                                                                                                                         \n",
    "# plot true counts                   \n",
    "ax.semilogy(m_bin_centers, corr,'C1',label='Corrected Sample')\n",
    "ax.semilogy(m_bin_centers, corr1,'C0',label='Corrected Counts')\n",
    "# Plot the true mass function   \n",
    "ax.semilogy(m_points,mass_func_arr(true_params,m_points),'C1--',linewidth=5,label='True Rate')\n",
    "ax.semilogy(m_points,mass_func_arr(res.params,m_points),'C2',label='Fit Rate')\n",
    "#ax.semilogy(m_bin_centers,2/(hist_true/(m_bins[1]-m_bins[0])),'k',label='Required for Corr')\n",
    "# plot mass sens\n",
    "#ax.semilogy(m_bin_centers, 1/mass_sens,'C4--',label='1/Mass Sens')\n",
    "# plot fitted function\n",
    "#ax.semilogy(m_points,mass_func_plot(res.params,m_points),'C2',label='Fit')\n",
    "ax.set_xlabel(r'$log(M/M_\\oplus)$')\n",
    "ax.set_ylabel(r'$dN/dlogM$')\n",
    "plt.legend()\n",
    "#ax.semilogy(m_points,mass_func(start,m_points))  \n",
    "#ax.semilogy(m_points,mass_func(res.x,m_points)) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Observed and Model Semi-Major Axis Distributions\n",
    "\n",
    "This section creates a plot that compares the observed distribution of planet semi-major axes (distances from their stars) to the predictions from your occurrence rate model.\n",
    "\n",
    "- It shows several lines:\n",
    "  - The raw (uncorrected) counts of planets per semi-major axis bin.\n",
    "  - The corrected counts, accounting for survey sensitivity and other factors.\n",
    "  - The \u201ctrue\u201d occurrence rate used for testing (if available).\n",
    "  - The best-fit occurrence rate from your model.\n",
    "- The y-axis is on a logarithmic scale to show a wide range of values.\n",
    "- The plot includes a legend to explain each line.\n",
    "\n",
    "> **Why do this?**  \n",
    ">\n",
    "> This plot lets you see how well your model matches the data for planet distances, after all corrections. It\u2019s a key check for the quality of your fit and the reliability of your results.\n",
    "\n",
    "**Example Output:**  \n",
    "Below is an example of what this plot should look like:\n",
    "\n",
    "![Example semi-major axis distribution comparison plot](ExampleFigures/Figure10.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should be analagous plot for SMA distribution, some testing and figuring needs to be done still\n",
    "# first of all should not be using mass_func_arr(), need a new version\n",
    "# maybe should be evaluating for fixed mass or SMA?\n",
    "\n",
    "# survey_sensitivity integrated over semi-major axis (a) \n",
    "sma_sens = scipy.integrate.simpson(survey_sensitivity.T,dx=m_bins[1]-m_bins[0],axis=0)\n",
    "#mass_sens = scipy.integrate.simpson(det_efficiency.T,dx=a_bins[1]-a_bins[0],axis=1)\n",
    "\n",
    "# plot the results of the LME     \n",
    "a_bin_centers = (a_bins[:-1]+a_bins[1:])/2\n",
    "# build mass histograms, relevant vars  \n",
    "hist,bins = np.histogram(np.log10(planet_sample['Planet_semimajoraxis']),bins=a_bins)\n",
    "hist_true,bins = np.histogram(np.log10(det['Planet_semimajoraxis']),bins=a_bins,weights=det['fwc'])\n",
    "\n",
    "# histograms of full sim results  \n",
    "# THIS LINE WAS RECENTLY HAD LAST COMMENT ADDED \n",
    "smaw, bins = np.histogram(np.log10(dat['Planet_semimajoraxis']),bins=a_bins,weights=dat['fwc'])\n",
    "smad, bins = np.histogram(np.log10(det['Planet_semimajoraxis']),bins=a_bins,weights=det['fwc'])\n",
    "\n",
    "#plot x points\n",
    "sma_points = np.linspace(-2,1,1000)  \n",
    "\n",
    "# plot mass\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "\n",
    "\n",
    "# plot uncorrected counts (units are counts per bin width)   \n",
    "ax.semilogy(a_bin_centers, hist/(a_bins[1]-a_bins[0]),'C0--',label='Raw Sample/bin')#*(m_bins[1]-m_bins[0]))                                                                                                  \n",
    "# plot true counts per bin width     \n",
    "ax.semilogy(a_bin_centers,( hist_true/(a_bins[1]-a_bins[0])),'C6--',label='Raw Counts/bin')#*(m_bins[1]-m_bins[0]))\n",
    "# calculate the corrected corrected observed counts  \n",
    "corr = hist  / sma_sens / (a_bins[1]-a_bins[0]) #/corr_fac                                                                                                                                               \n",
    "# calculate the true injected mass function  \n",
    "true_x = mass_func_arr(true_params,a_bin_centers)\n",
    "# calculate the corrected true counts                                                                                                                                                                     \n",
    "corr1 = hist_true / sma_sens / (a_bins[1]-a_bins[0]) #/ corr_fac                                                                                                                                         \n",
    "# plot true counts                   \n",
    "ax.semilogy(a_bin_centers, corr,'C1',label='Corrected Sample')\n",
    "ax.semilogy(a_bin_centers, corr1,'C0',label='Corrected Counts')\n",
    "# Plot the true mass function   \n",
    "ax.semilogy(sma_points,mass_func_arr(true_params,sma_points),'C1--',linewidth=5,label='True Rate')\n",
    "ax.semilogy(sma_points,mass_func_arr(res.params,sma_points),'C2',label='Fit Rate')\n",
    "#ax.semilogy(m_bin_centers,2/(hist_true/(m_bins[1]-m_bins[0])),'k',label='Required for Corr')\n",
    "# plot mass sens\n",
    "#ax.semilogy(m_bin_centers, 1/mass_sens,'C4--',label='1/Mass Sens')\n",
    "# plot fitted function\n",
    "#ax.semilogy(m_points,mass_func_plot(res.params,m_points),'C2',label='Fit')\n",
    "ax.set_xlabel(r'$log(a/AU)$')\n",
    "ax.set_ylabel(r'$dN/dloga$')\n",
    "plt.legend()\n",
    "#ax.semilogy(m_points,mass_func(start,m_points))  \n",
    "#ax.semilogy(m_points,mass_func(res.x,m_points)) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnostic Plots for Correction Factors\n",
    "\n",
    "This section creates several diagnostic plots to help you understand and debug the correction factors used in your analysis.\n",
    "\n",
    "- It plots different combinations of your mass sensitivity (`mass_sens`), corrected counts (`corr1`), and other related quantities.\n",
    "- The lines show how these factors relate to each other across the mass bins.\n",
    "- The y-axis is on a logarithmic scale, and the plot includes a legend for clarity.\n",
    "\n",
    "> **Why do this?** \n",
    "> \n",
    "> These plots are mainly for troubleshooting and checking that your correction factors behave as expected.  \n",
    "They help you spot any issues or inconsistencies in your sensitivity calculations or data corrections, making sure your final results are trustworthy.\n",
    "\n",
    "**Tip:**  \n",
    "You can use these plots to compare different ways of normalizing or correcting your data, and to ensure that your sensitivity and correction factors are working as intended.\n",
    "\n",
    "**Example Output:**  \n",
    "Below is an example of what this plot should look like:\n",
    "\n",
    "![Example diagnostic correction factor plot](ExampleFigures/Figure11.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just some diagnostic plots when I was trying to figure out what was going on with corrections\n",
    "\n",
    "#mass_sens = scipy.integrate.simpson(survey_sensitivity,dx=a_bins[1]-a_bins[0],axis=0)\n",
    "fig,ax = plt.subplots()\n",
    "ax.semilogy(m_bin_centers,2/corr1, label='Corr for hist/mass_sens')\n",
    "ax.semilogy(m_bin_centers,mass_sens, label='mass_sens')\n",
    "ax.semilogy(m_bin_centers,1/mass_sens,label='1/mass_sens')\n",
    "ax.semilogy(m_bin_centers,2/( hist_true/(m_bins[1]-m_bins[0])),label='2/hist')\n",
    "ax.set_xlabel(r'$log(M/M_\\oplus)$')\n",
    "ax.set_ylabel(r'$dN/dlogM$')\n",
    "plt.legend()\n",
    "#ax.semilogy(m_points,mass_func(start,m_points))  \n",
    "#ax.semilogy(m_points,mass_func(res.x,m_points)) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-fitting with MCMC for Posterior Collecion\n",
    "\n",
    "---\n",
    "\n",
    "This is the part that tell us how good our estimate will actually be. \n",
    "\n",
    "### About MCMC\n",
    "\n",
    "Markov Chain Monte Carlo (MCMC) is a powerful statistical technique used to explore the range of possible model parameters that fit your data.  \n",
    "Instead of just finding the single \u201cbest\u201d answer, MCMC helps you map out the full landscape of solutions, showing you the uncertainties and correlations between parameters.\n",
    "\n",
    "**Why use MCMC?**\n",
    "- It allows you to estimate not just the most likely values of your model parameters, but also their uncertainties and how they might be related.\n",
    "- MCMC is especially useful when your model is complex or when the likelihood surface is not a simple shape.\n",
    "- It provides a way to sample from the full \u201cposterior distribution\u201d of your parameters, giving you a much richer understanding of your results.\n",
    "\n",
    "**In this notebook:**  \n",
    "We use the [`emcee`](https://emcee.readthedocs.io/en/stable/) Python package, which is a popular and user-friendly tool for running MCMC analyses.\n",
    "\n",
    "For more details, see the [emcee documentation](https://emcee.readthedocs.io/en/stable/).\n",
    "\n",
    "### Setting Up the MCMC Sampler\n",
    "\n",
    "This section sets up the parameters and starting positions for running a Markov Chain Monte Carlo (MCMC) analysis.\n",
    "\n",
    "- It defines the number of walkers (`n_walkers`), the number of steps each walker will take (`n_steps`), and the number of initial steps to discard as \u201cburn-in\u201d (`n_burn`).\n",
    "- It uses the results from your previous fit (`res.params`) as the starting point for the MCMC chains.\n",
    "- It creates a set of slightly randomized starting positions for each walker, based on the best-fit values and a small spread (`starting_sigma`).\n",
    "- It prepares variables for tracking the progress and convergence of the MCMC run (like `autocorr` and `old_tau`).\n",
    "\n",
    "> **Why do this?**  \n",
    ">\n",
    "> MCMC is used to explore the range of model parameters that fit your data, giving you uncertainties and correlations.  \n",
    "> Setting up the sampler with good starting points helps the chains converge faster and gives you more reliable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### MCMC ####\n",
    "# Was set up for iterative redraws of planet sample, keeping some of the framework for that\n",
    "i = 0\n",
    "filename = \"mcmc_\"+str(i)+\".par4.nomaglim.invsens.h5\"\n",
    "n_walkers = 15\n",
    "n_steps = 1000\n",
    "n_burn = 100\n",
    "\n",
    "#set-up\n",
    "logC_start = res.params['logC']\n",
    "n_start = res.params['n']\n",
    "m_start = res.params['m']\n",
    "\n",
    "starting_mean = np.array([logC_start,n_start,m_start])\n",
    "starting_sigma = 0.01*np.ones(len(starting_mean))\n",
    "starting = [(starting_mean + starting_sigma * np.random.randn(len(starting_mean))) for i in range(n_walkers)]\n",
    "\n",
    "index = 0\n",
    "autocorr = np.empty(n_steps)\n",
    "old_tau = np.inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the MCMC Sampler and Backend\n",
    "\n",
    "This section sets up the MCMC sampler and its backend for saving results.\n",
    "\n",
    "- It prints a message to show the process is starting.\n",
    "- It deletes any old MCMC result file with the same name, so you start fresh (remove this line if you want to continue or stack runs).\n",
    "- It creates an HDF5 backend using `emcee`, which will save the MCMC samples to disk as the sampler runs.\n",
    "- It sets up the sampler with the number of walkers, number of parameters, the probability function ([`ln_prob`](#what-does-ln_prob-do)), and the arguments your model needs.\n",
    "\n",
    "**How to use this cell:**\n",
    "1. Make sure the `filename` and folder (`data_penny/`) exist and are correct for your project.\n",
    "2. If you want to keep results from previous runs, comment out or remove the `os.remove(...)` line.\n",
    "3. Run this cell before starting the actual MCMC sampling loop.\n",
    "\n",
    "> **Why do this?**  \n",
    ">\n",
    "> Saving the MCMC samples to disk lets you pause and resume runs, and makes it easy to analyze the results later without rerunning the whole chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('starting')\n",
    "\n",
    "# delete old file, can be removed if you want to stack MCMC runs\n",
    "os.remove(\"data_penny/\"+filename)\n",
    "\n",
    "backend = emcee.backends.HDFBackend(\"data_penny/\"+filename)\n",
    "\n",
    "n_parameters = len(starting_mean)\n",
    "\n",
    "sampler = emcee.EnsembleSampler(n_walkers, n_parameters, ln_prob, args=(planet_sample, survey_sensitivity),backend=backend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the MCMC Sampling Loop and Checking for Convergence\n",
    "\n",
    "This cell runs the MCMC sampler to explore the parameter space and checks for convergence as it goes.\n",
    "\n",
    "- It starts a timer to measure how long the sampling takes.\n",
    "- It runs the sampler for up to `n_steps` iterations, starting from the initial positions.\n",
    "- Every 100 steps, it:\n",
    "  - Prints the current iteration and checks the \u201cautocorrelation time\u201d (a measure of how independent the samples are).\n",
    "  - Stores the average autocorrelation time for later analysis.\n",
    "  - Checks if the chains have converged:\n",
    "    - Convergence is assumed if the total number of steps is much larger than the autocorrelation time (at least 50 times), and if the autocorrelation time has stabilized (changed by less than 1%).\n",
    "    - If converged, the loop breaks early to save time.\n",
    "- The timer is stopped at the end to record how long the sampling took.\n",
    "\n",
    "> **Why do this?**  \n",
    ">\n",
    "> This approach ensures you don\u2019t waste time running the sampler longer than needed, and helps you get reliable, independent samples from the posterior distribution.  \n",
    "> Monitoring the autocorrelation time is a standard way to check if your MCMC chains have \u201cmixed\u201d and are sampling the parameter space effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timeit.default_timer()\n",
    "for sample in sampler.sample(starting, iterations=n_steps,progress=False):\n",
    "    #print(sampler.iteration)\n",
    "    # Only check convergence every 100 steps\n",
    "    if sampler.iteration % 100:\n",
    "        continue\n",
    "\n",
    "    #ipdb.set_trace()\n",
    "    print('Checking autocorr time at step: ',sampler.iteration)\n",
    "    # Compute the autocorrelation time so far\n",
    "    # Using tol=0 means that we'll always get an estimate even\n",
    "    # if it isn't trustworthy\n",
    "    tau = sampler.get_autocorr_time(tol=0)\n",
    "    autocorr[index] = np.mean(tau)\n",
    "    index += 1\n",
    "\n",
    "    # Check convergence\n",
    "    converged = np.all(tau * 50 < sampler.iteration)\n",
    "    converged &= np.all(np.abs(old_tau - tau) / tau < 0.01)\n",
    "    print('ACT at iter: '+str(autocorr[index-1]) +' at ' +str(int(sampler.iteration)))\n",
    "    if converged:\n",
    "        break\n",
    "    old_tau = tau\n",
    "\n",
    "stop = timeit.default_timer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCMC Diagnostics\n",
    "\n",
    "There really needs to be an additional cell here to plot the sampler chains.\n",
    "\n",
    "* If walker trajectories (plot sampler.get_chain()) look like \"hairy caterpillars,\" chains are mixed\n",
    "* If they resemble \"flat lines,\" chains are stuck (likely bad initialization or model mismatch)\n",
    "* If they look like satifying overlapping scribbles with a consistent width, the sampler has converged and is effectively sampling the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCMC Diagnostics\n",
    "\n",
    "# [ ] plot the sampler chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing MCMC Results with a Corner Plot\n",
    "\n",
    "This section visualizes the results of your MCMC run and shows how the model parameters are distributed and correlated.\n",
    "\n",
    "- It prints a summary of how long the MCMC run took and how many samples were collected.\n",
    "- It flattens the MCMC chains (removing the burn-in) to get a set of independent samples.\n",
    "- It creates a \u201ccorner plot\u201d using the [`corner`](https://corner.readthedocs.io/en/latest/) library, which shows:\n",
    "  - The distribution (posterior) for each parameter (e.g., `logC`, `n`, `m`).\n",
    "  - The correlations between parameters (e.g., how `n` and `logC` are related).\n",
    "- It also plots many possible occurrence rate curves from the posterior, so you can see the range of models that fit your data.\n",
    "\n",
    "> **Pro Tips for Using the Corner Plot:**\n",
    "> - **Look for smooth, bell-shaped (Gaussian) distributions** for each parameter. This means your MCMC has likely converged and is sampling the true posterior.\n",
    "> - **Check for correlations:** If you see slanted or curved shapes in the 2D plots (like between `n` and `logC`), it means those parameters are correlated\u2014changing one affects the other.\n",
    "> - **Watch for problems:** If you see weird shapes (multiple peaks, sharp edges, or \u201cwalls\u201d), it could mean:\n",
    ">   - The chains haven\u2019t converged (try running longer).\n",
    ">   - There are issues with your model or priors.\n",
    ">   - The parameter space is not well-explored (try more walkers or a different starting point).\n",
    "\n",
    "**Example Output:**  \n",
    "\n",
    "```\n",
    "Completed after  1000  samples in  0.011527339476160704  hours.\n",
    "```\n",
    "\n",
    "Below is an example of what a good corner plot should look like, with nice Gaussian posteriors and visible correlations:\n",
    "\n",
    "![Example MCMC corner plot](ExampleFigures/Figure12.png)\n",
    "\n",
    "**Example Model Curves:**  \n",
    "The plot below shows many possible occurrence rate curves drawn from the MCMC posterior, illustrating the range of models that fit your data:\n",
    "\n",
    "![Example model curves from MCMC](ExampleFigures/Figure13.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Corner Plot ####\n",
    "\n",
    "print('Completed after ', sampler.iteration, ' samples in ',(stop-start)/60/60 ,' hours.')\n",
    "flat_samples = sampler.get_chain(discard=n_burn, flat=True)\n",
    "# make sure this matches the labels of parameters you want to plot\n",
    "labels = ['logC','n','m']#,'m']\n",
    "fig = corner.corner(flat_samples, labels=labels, truths=starting_mean)\n",
    "plt.show()\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "ax.semilogy(m_points,mass_func_arr(true_params,m_points),label='True',zorder=10000)\n",
    "inds = np.random.randint(len(flat_samples), size=100)\n",
    "for ind in inds:\n",
    "    sample = flat_samples[ind]\n",
    "    #print(sample)\n",
    "    ax.semilogy(m_points,mass_func_arr(sample,m_points),'C1',alpha=0.1)\n",
    "    #plt.plot(x0, np.dot(np.vander(x0, 2), sample[:2]), \"C1\", alpha=0.1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Interpret the Results\n",
    "\n",
    "Congratulations! You\u2019ve worked through the full analysis pipeline. Here\u2019s how to make sense of your results and what to do next:\n",
    "\n",
    "### 1. **Best-Fit Model Parameters**\n",
    "- The notebook provides the best-fit values for your occurrence rate model (e.g., normalization, mass slope, semi-major axis slope).\n",
    "- **Interpretation:** These numbers tell you how common different types of planets are in your survey, and how occurrence rates change with planet mass and distance.\n",
    "\n",
    "### 2. **Uncertainties and Correlations**\n",
    "- The MCMC results (corner plots) show the range of parameter values that are consistent with your data, as well as how parameters are correlated.\n",
    "- **Interpretation:** Narrow, bell-shaped posteriors mean your data strongly constrain the parameters. Wide or oddly-shaped posteriors mean more uncertainty or possible model issues.\n",
    "- **Correlations** (slanted ellipses in the corner plot) mean that changing one parameter can be compensated by changing another.\n",
    "\n",
    "### 3. **Model vs. Data Plots**\n",
    "- The comparison plots show how well your model matches the observed distributions of planet mass and semi-major axis.\n",
    "- **Interpretation:** If the \u201cfit\u201d and \u201ctrue\u201d lines closely follow the corrected data, your model is a good description of the observed population.\n",
    "\n",
    "### 4. **Survey Sensitivity**\n",
    "- The sensitivity maps and summary plots show where your survey is most and least sensitive to finding planets.\n",
    "- **Interpretation:** Be cautious about drawing conclusions in regions where your sensitivity is low\u2014your survey may have missed planets there.\n",
    "\n",
    "### 5. **Physical Meaning**\n",
    "- Use the best-fit parameters and their uncertainties to answer scientific questions, such as:\n",
    "  - How common are Earth-mass planets at 1 AU?\n",
    "  - Does planet occurrence increase or decrease with mass or distance?\n",
    "  - Are your results consistent with previous studies?\n",
    "\n",
    "### 6. **Caveats and Next Steps**\n",
    "- **Check for convergence:** Make sure your MCMC chains have mixed and your posteriors look reasonable.\n",
    "- **Compare to example outputs:** If your results look very different, double-check your data and settings.\n",
    "- **Consider model limitations:** If your model doesn\u2019t fit well, try a different functional form or add more parameters.\n",
    "- **Calculate \u03b7_Earth:** Use your final, fitted occurrence rate model to calculate the actual fraction of stars hosting Earth-like planets. This involves integrating your model `d\u00b2N / dmdS` over a defined \"Earth-like\" box in mass and insolation space.\n",
    "\n",
    "---\n",
    "\n",
    "**In summary:**  \n",
    "Your results provide a quantitative, uncertainty-aware measurement of planet occurrence rates in your survey.  \n",
    "Use these findings to inform further research, compare with other surveys, or refine your models.\n",
    "\n",
    "If you have questions or want to try new models or data, you can always revisit earlier steps in the notebook.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boss Battle: Fitting the Right Model\n",
    "Okay, time for the final check. The goal here is to prove the notebook can correctly find the \"true\" parameters you used to create your simulation. But this only works if you fit the same kind of model that you simulated.\n",
    "\n",
    "So, first, the most important question: What kind of model did you use for your simulation?\n",
    "\n",
    "### Path A: If you simulated a SIMPLE Power Law...\n",
    "\n",
    "Then congratulations, you've already won. The main part of the notebook, as it is, is designed to fit a simple power law.\n",
    "\n",
    "Your \"Boss Battle\" was just running the MCMC and confirming that the final plots show the \"Fit Rate\" perfectly matching your \"True Rate.\" If they line up and your corner plot looks good, you have successfully validated the pipeline for a simple model. The uncertainties you got are the RST-science-requirement secret sauce. You're done. Go have a drink.\n",
    "\n",
    "### Path B: If you simulated a BROKEN Power Law...\n",
    "\n",
    "Okay, now it's a real fight. Your simulation is more complex, and you need to prove the notebook can handle it. This is how you start getting results that look like the ones in the [Suzuki et al. (2012)](https://ui.adsabs.harvard.edu/abs/2016ApJ...833..145S/abstract) and [Penny et al. (2019)](https://ui.adsabs.harvard.edu/abs/2019ApJS..241....3P/abstract) papers.\n",
    "\n",
    "#### Your Mission:\n",
    "\n",
    "1.  **Activate the Complex Parameters:** In the **\"Setting Up True Model Parameters for Testing\"** cell, you need to switch from the simple model to the full one.\n",
    "    * **Comment out** the `simplified version` of the parameters.\n",
    "    * **Uncomment** the \"full functional fitting\" `true_parameters` dictionary. It has more pieces, like a `logMbreak` and different slopes (`n1`, `n2`). Make sure these values match the simulation you're trying to test.\n",
    "\n",
    "2.  **Upgrade Your Fitting Functions:** The simple functions won't work with this new model. You will need to modify them to handle a broken power law.\n",
    "    * **In `mass_func_arr`**: You need to change the logic to use one slope (`n1`) for planets *below* the break mass and another slope (`n2`) for planets *above* it. It will look something like this conceptually:\n",
    "        ```python\n",
    "        # Conceptual change for mass_func_arr\n",
    "        if M_planet < M_break:\n",
    "            # Use the first slope\n",
    "            rate = C1 * (M_planet / M_ref1)**n1\n",
    "        else:\n",
    "            # Use the second slope\n",
    "            rate = C2 * (M_planet / M_ref2)**n2\n",
    "        ```\n",
    "    * **In `ln_prior`**: You have to update the priors to check the allowed ranges for all the new parameters in your `true_parameters` dictionary (`logMbreak`, `n1`, `n2`, etc.).\n",
    "\n",
    "3.  **Adjust the MCMC Setup**: In the **\"Setting Up the MCMC Sampler\"** cell, you'll need to update the `starting_mean` array and the `labels` for the corner plot to include all the new parameters you're fitting.\n",
    "\n",
    "This is a significant step up, so don't be surprised if it takes a few tries. But getting this to work is how you go from just testing the code to doing real science with it. Good luck.\n",
    "\n",
    "### Path C: The data are real...\n",
    "\n",
    "So, you're done with simulations and are using actual, real-sky survey data. This is no longer a test; this is for all the marbles. The goal now isn't to recover a known truth, but to *discover* it.\n",
    "\n",
    "**Your Mission:**\n",
    "\n",
    "1.  **There Are No \"True\" Parameters:** The `true_params` dictionary is now irrelevant. You can comment it out or delete it entirely. The MCMC fit you get at the end *is* the result. The \"truths\" you were plotting in the corner plot don't exist anymore.\n",
    "\n",
    "2.  **Model Selection is Key:** Since you don't know the true shape of the planet distribution, you may need to test multiple models. You should probably:\n",
    "    * Fit the **simple power-law model** first.\n",
    "    * Then, fit the **broken power-law model** (like in Path B).\n",
    "\n",
    "3.  **Compare the Models:** How do you know which model is better? You have to compare them. [The Suzuki et al. (2016)](https://ui.adsabs.harvard.edu/abs/2016ApJ...833..145S/abstract) paper does exactly this. They use a statistical test (the \"Bayes factor\") to show that the broken power-law model is a much better fit to their data than the simple one. You'll need to do a similar comparison to justify using a more complex model.\n",
    "\n",
    "4.  **The MCMC Result IS The Answer:** The final corner plot and the parameter distributions it shows are your scientific result.\n",
    "    * The **peak** of each distribution is your best measurement for that parameter.\n",
    "    * The **width** of the distribution is your uncertainty, or your error bars.\n",
    "\n",
    "When you're done, you won't be saying \"my code recovered the right answer.\" You'll be saying, \"we measured the slope of the planet mass function to be $n$ with an uncertainty of $x$.\"\n",
    "\n",
    "This is the whole point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're right. My apologies. My explanation was too simplistic\u2014you've got a sharper definition. It's not about *time*, it's about *space*. A subtle but crucial difference. Let's fix it.\n",
    "\n",
    "It's sloppy to leave an imprecise explanation in there. We'll rewrite that whole appendix to be more exact.\n",
    "\n",
    "Replace the previous appendix with this one. It's better.\n",
    "\n",
    "***\n",
    "\n",
    "## Appendix A: De-Mystifying the Math\n",
    "\n",
    "If you're wondering what \"detection efficiency\" or \"weights\" are, you're in the right place. This is the boring-but-important stuff that makes the whole analysis work.\n",
    "\n",
    "### What is Detection Efficiency?\n",
    "\n",
    "In a perfect world, our telescope would see every single planet that exists in the survey fields. We don't live in a perfect world. The telescope has blind spots, it's better at finding big planets than small ones, and it's better at finding planets at certain distances.\n",
    "\n",
    "**Detection Efficiency** is simply a measure of how good the survey is at finding a specific type of planet.\n",
    "\n",
    "Think of it like this: Imagine you're looking for red marbles in a huge sandbox, but you're wearing sunglasses that make it harder to see the color red.\n",
    "* In some parts of the sandbox with bright sunlight, your detection efficiency for red marbles is high.\n",
    "* In shady parts, your detection efficiency is low.\n",
    "* If you're also worse at spotting small marbles than big ones, your efficiency is lower for the small ones.\n",
    "\n",
    "Your final \"detection efficiency map\" (like the ones in the notebook) is like a chart showing your chances of finding a marble of a certain size in a certain spot. We calculate it for the telescope using this simple formula:\n",
    "\n",
    "$$\\text{Detection Efficiency} = \\frac{\\text{Number of Planets We ACTUALLY Detected}}{\\text{Total Number of Planets That EXISTED in the Survey Volume}}$$\n",
    "\n",
    "This is the same as the `det_eff = N_det / N_dat` you see in the notes. We need this map to correct for all the planets the telescope inevitably missed, otherwise we'd get the wrong answer for how common planets are.\n",
    "\n",
    "### How Do the Weights Work?\n",
    "\n",
    "\"Weight\" is just a number that tells us how important or likely a single simulated event is. In this notebook, the weights get adjusted in a few steps.\n",
    "\n",
    "1.  **The Base Weight (`final_weight` from the file):** Each event from the Gulls simulation comes with a starting weight. This number represents how likely that specific microlensing event is to happen in the universe, based on the simulation's Galactic model. A high weight means a more common type of event.\n",
    "\n",
    "    ``` final_weight \u221d (probability of lens-source alignment) \u00d7 (source star density) ```\n",
    "\n",
    "2.  **The Geometric Correction (`covfac`):** The simulation generates events across a simple, square area of the sky. But the Roman telescope's camera has a specific footprint with gaps between the detectors. The **covering factor (`covfac`)** is the fraction of the simulated square that is *actually* covered by one of Roman's detectors. If the Roman footprint only covers 80% of that simulation area, the `covfac` is 0.8. We multiply the event's weight by this factor to correct for the part of the simulated area that was never even observed.\n",
    "\n",
    "3.  **The Final Corrected Weight (`fwc`):** This is the most important weight for the analysis. It's the `Base Weight` \u00d7 `Geometric Correction`. When we sum up the `fwc` of all the *detected* planets (`det`), it gives us our best estimate of the true number of planets that exist in that category. This is what we use to calculate the \"Corrected Counts\" in the final plots.\n",
    "\n",
    "4.  **The Temporary Model Weight (for drawing a sample):** This one is confusing, but it's only used for the validation test. To create a realistic *mock* sample of planets to test the MCMC, we do one more multiplication:\n",
    "    `fwc` \u00d7 (value from the `true_params` occurrence rate model)\n",
    "\n",
    "    This creates a temporary set of weights used *only* in the `draw_sample` function. The goal is to create a fake dataset of planets that we know follows a specific power law, so we can see if the MCMC can correctly recover the parameters of that law. Once the sample is drawn, these temporary weights are thrown away."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eta-earth-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
